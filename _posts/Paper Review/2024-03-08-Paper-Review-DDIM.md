---
layout: post
title: "[Generative Model] DDIM : Denoising Diffusion Implicit Models"
author: kjy
date: 2024-03-08 15:00:00 +09:00
categories: [Deep Learning, Paper Review]
tags: [Deep Learning, Paper Review, Computer Vision, Generative Model]
comments: true
toc: true
math: true
---

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [DDIM : Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) ë…¼ë¬¸ì—ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

ë…¼ë¬¸ì„ ì „ì²´ì ìœ¼ë¡œ ì½ê³  ì‹¶ìœ¼ì‹  ë¶„ì€ [Paper Reading](https://jjjuuuun.github.io/posts/Paper-Reading-DDIM/)ìœ¼ë¡œ ì´ë™í•´ì£¼ì„¸ìš”!

## 1. DDIM ë“±ì¥ ë°°ê²½

ìµœê·¼ì— iterative generative models(DDPM, NCSN)ì— ëŒ€í•œ ì—°êµ¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. Iterative generative modelsì€ GANsê³¼ ë¹„êµí–ˆì„ ë•Œ í•™ìŠµì´ ì‰½ì§€ë§Œ ë†’ì€ í’ˆì§ˆì˜ sampleì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ë§Œ sampleì„ ìƒì„±í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì´ GANsë³´ë‹¤ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦°ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¨ì ì„ ë³´ì™„í•˜ê³ ì í•˜ëŠ” ê²ƒì´ DDIMì…ë‹ˆë‹¤.

DDIMì€ DDPMì—ì„œ ì‚¬ìš©ë˜ëŠ” Markov chainê³¼ì •ì„ ìˆ˜ì •í•˜ì—¬ sample ìƒì„± ì†ë„ë¥¼ ë†’ì´ê³ ì í–ˆìŠµë‹ˆë‹¤.

## 2. DDIMì˜ íŠ¹ì§•

â€» [DDIM ì •ë¦¬](#3-ddim-ì •ë¦¬)ì™€ ì´ì–´ì§‘ë‹ˆë‹¤.

1. Forward processì—ì„œ ë” ì´ìƒ Markov chainì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ([ë°”ë¡œ ë³´ê¸°](#31-non-markov-process))
2. DDPMê³¼ ê°™ì€ objective functionì„ ì‚¬ìš©í•´ì„œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ([ë°”ë¡œ ë³´ê¸°](#33-objective-function-ì •ì˜))
3. Sampling ì†ë„ê°€ DDPMë³´ë‹¤ ë§ì´ ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤. ([ë°”ë¡œ ë³´ê¸°](#35-sampling-ì†ë„-í–¥ìƒ))
4. Training imageì˜ ìœ ì¼í•œ latentê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ([ë°”ë¡œ ë³´ê¸°](#32-generative-process-ì •ì˜))
   - $\sigma\_t$ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•¨ìœ¼ë¡œì¨ sampleì€ ê³ ì •ëœ latentì—ì„œ ìƒì„±ì´ ë©ë‹ˆë‹¤.
   - í•˜ë‚˜ì˜ latentë¡œ ê³ ì •ë˜ë©´ì„œ timestepsê³¼ ìƒê´€ì—†ì´ high-level image featureë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
   - Latentë¥¼ ê°€ì§€ê³  GANsì²˜ëŸ¼ semantically meaningful interpolationì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## 3. DDIM ì •ë¦¬

### 3.1 Non-Markov process

DDIMì—ì„œëŠ” sampling ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ DDPMì˜ forward processë¥¼ Non-Markov processë¡œ ì¬ì •ì˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

$$q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_0) := q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod^T_{t=2}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)\tag{1}$$

ìœ„ì˜ ìˆ˜ì‹ì²˜ëŸ¼ forward processë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ì‚¬ì „ ì •ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. (êµ¬ì²´ì ì¸ ìˆ˜ì‹ ì¦ëª…ì€ [Paper Reading](https://jjjuuuun.github.io/posts/Paper-Reading-DDIM/)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.)

$$
\begin{align}
&q_\sigma(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_t}\mathbf{x}_0, (1-\alpha_t)\mathrm{I})\tag{2}\\
&q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}\bigg(\sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\bigg)\qquad (t > 1)\tag{3}\\
&q_\sigma(\mathbf{x}_{t}|\mathbf{x}_{t-1}, \mathbf{x}_0) = q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0) \times \frac{q_{\sigma}(\mathbf{x}_t|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)}\tag{4}
\end{align}
$$

### 3.2 Generative process ì •ì˜

- ì‹ $2$ë¥¼ ë³€ê²½í•´ì„œ í•™ìŠµëœ ëª¨ë¸ë¡œë¶€í„° $\mathbf{x}\_0$ë¥¼ ì˜ˆì¸¡

  $$
  \begin{align}
  f_{\theta}^{(t)}(\mathbf{x}_{t})
  &=\ \hat{\mathbf{x}}_0 \\
  &=\ (\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})) / \sqrt{\alpha_{t}} \tag{5}
  \end{align}
  $$

- ì‹ $3$ì˜ $\mathbf{x}\_0$ë¶€ë¶„ì— ìœ„ì—ì„œ ì˜ˆì¸¡í•œ $\mathbf{x}\_0$ì„ ëŒ€ì…

  $$
  \begin{align}
  q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \hat{\mathbf{x}}_{0})
  &= q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t)) \\
  &= \mathcal{N}\big(\sqrt{\alpha_{t-1}}f_{\theta}^{(t)}(\mathbf{x}_{t}) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}f_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\big) \tag{6}
  \end{align}
  $$

- ìµœì¢… generative process ì •ì˜

  - ì‹ $3$ì—ì„œ $t$ì˜ ë²”ìœ„ê°€ $1$ë³´ë‹¤ í° ê²½ìš°ì´ë¯€ë¡œ $t=1$ì¸ ê²½ìš° ë”°ë¡œ ì •ì˜ë¥¼ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
  - Generative processê°€ ëª¨ë“  $t$ì—ì„œ ì‘ë™í•˜ê²Œ í•˜ê¸°ìœ„í•´ì„œ $t=1$ì¼ ë•Œ gaussian noiseë¥¼ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.
  - ê·¸ëŸ¬ë©´ ì•„ë˜ì™€ ê°™ì´ ìµœì¢… generative processë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  $$
  p_{\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t})=
  \begin{cases}
  \mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1),\ \sigma_1^2\mathrm{I}), & \text{if } t = 1 \\
  q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t)), & \text{otherwise}
  \end{cases}
  \tag{7}
  $$

### 3.3 Objective function ì •ì˜

Forward processì˜ ì •ì˜ê°€ ë‹¬ë¼ì§ì— ë”°ë¼ì„œ generative processë„ ë°”ë€ ê²ƒ ì²˜ëŸ¼ objective function ë˜í•œ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ $\mathbb{E}\left[-\log\ p\_\theta(\mathbf{x}\_0)\right]$ì„ ìµœì†Œí™” í•˜ê³ ì í•œ ê²ƒì€ DDPMê³¼ ë‹¤ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.

ìµœì¢… objective function ì‹ì„ DDPMê³¼ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.

$$
\begin{align}
&L = \sum^T_{t=1}\frac{\beta_t^2}{2\sigma_t^2\cdot\alpha_t\cdot(1-\bar{\alpha}_t)}\mathbb{E}\bigg[\big\lVert  \epsilon_t\ -\ \epsilon^{(t)}_\theta(\mathbf{x}_t)\big\rVert^2_2  \bigg]& \cdots\quad \text{DDPM} \tag{8}\\
&J_\sigma = \sum^T_{t=1}\frac{1-\alpha_t}{2\sigma^2_t\alpha_t}\mathbb{E}\bigg[\big\lVert  \epsilon_t\ -\ \epsilon^{(t)}_\theta(\mathbf{x}_t)\big\rVert^2_2  \bigg]& \cdots\quad \text{DDIM}\tag{9}
\end{align}
$$

DDPMê³¼ DDIMì—ì„œ ì‚¬ìš©í•˜ëŠ” notationì´ ë‹¤ë¥´ê¸°ëŠ” í•˜ì§€ë§Œ ê·¸ ì°¨ì´ëŠ” ìƒìˆ˜ë°°ì˜ ì°¨ì´ë§Œ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ DDIMì˜ objectiveë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$J_\sigma(\epsilon_\theta) = L(\epsilon_\theta) + C\tag{10}$$

ì²˜ìŒ ì‹œì‘ì ì€ ê°™ê³  ì¤‘ê°„ê³¼ì •ì€ ë‹¬ëì„ì§€ ëª°ë¼ë„ DDPMì˜ objective functionë¥¼ DDIMì—ì„œ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.

### 3.4 $\sigma$ê°€ ë¬´ì—‡ì¸ê°€?

Objective functionê³¼ samplingì„ ìœ„í•œ ì‹ $6$ì— $\sigma$ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ objective functionì—ì„œëŠ” DDPMê³¼ ê°™ì€ objective functionì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë¬´ì‹œí•˜ê³  ì‹ $6$ë§Œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ì‹ $6$ì„ $\mathbf{x}_{t-1}$ì— ê´€í•œ ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\begin{align}
q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \hat{\mathbf{x}}_{0}) &= q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t)) & \qquad \\

&= \mathcal{N}\big(\sqrt{\alpha_{t-1}}f_{\theta}^{(t)}(\mathbf{x}_{t}) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}f_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\big) & \qquad \\ &= \sqrt{\alpha_{t-1}}f_{\theta}^{(t)}(\mathbf{x}_{t}) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}f_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{1-\alpha_t}} + \sigma_t \epsilon_t & \qquad \\

&= \sqrt{\alpha_{t-1}}\Bigg(\frac{\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}} \Bigg) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}\big(\frac{\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}} \big)}{\sqrt{1-\alpha_t}} + \sigma_t \epsilon_t & \qquad \\

&= \sqrt{\alpha_{t-1}}\underset{\text{"predicted }\mathbf{x}_{0}\text{"}}{\underbrace{\bigg(\frac{\mathbf{x}_{t} - \sqrt{1-\alpha_{t}}\epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}}\bigg)}}\ +\ \underset{\text{"direction pointing to }\mathbf{x}_{t}\text{"}}{\underbrace{\sqrt{1 - \alpha_{t-1} - \sigma^{2}_{t}}\ \cdot\ \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}}\ +\ \underset{\text{random noise}}{\underbrace{\sigma_{t}\epsilon_{t}}} & \qquad \tag{11}\\

\end{align}
$$

ë§Œì•½ $\sigma$ê°€ $0$ì´ë©´ random noiseì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì´ $0$ì´ ë˜ë¯€ë¡œ randomí•˜ì§€ ì•Šê²Œ samplingì´ ê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰, $\sigma$ëŠ” stochasticí•œ ì •ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” parameterë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $\sigma$ê°€ $0$ì¼ ë•Œ DDIMì´ë¼ê³  ë¶€ë¥´ë©° random í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ sampleì€ ê³ ì •ëœ latentë¡œë¶€í„° ìƒì„±ë˜ê²Œ ë©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ random noiseì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì—ì„œ $\sigma$ê°€ DDPMì—ì„œì˜ $\tilde{\beta}$ì™€ ê°™ë‹¤ë©´ DDPMì´ ë©ë‹ˆë‹¤. DDPMì—ì„œì˜ $\tilde{\beta}$ë¥¼ DDIM notationìœ¼ë¡œ ë°”ê¾¸ë©´ $\sigma_{t} = \sqrt{(1-\alpha_{t-1})/(1-\alpha_{t})}\sqrt{1-\alpha_{t}/\alpha_{t-1}}$ì´ê³  ì´ ë•Œ DDPMì˜ samplingê³¼ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3.5 Sampling ì†ë„ í–¥ìƒ

Forward processê°€ ë” ì´ìƒ Markov processë¥¼ ë”°ë¥´ì§€ ì•Šê¸° ë•Œë¬¸ì— $q\_\sigma(\mathbf{x}\_t\|\mathbf{x}\_0)$ë§Œ ê³ ì •ë˜ì–´ ìˆë‹¤ë©´ $1 \sim T$ ëª¨ë‘ sampling í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

ì™œëƒí•˜ë©´ $q\_\sigma(\mathbf{x}\_t\|\mathbf{x}\_0)$ë§Œ ê³ ì •ë˜ì–´ ìˆë‹¤ë©´ ì‹ $11$ì„ $t-1$ì´ ì•„ë‹Œ ë‹¤ë¥¸ $t-2$ ë˜ëŠ” $t-3$ ë“±ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ forward processì˜ ì—­ì´ generative processì´ê¸° ë•Œë¬¸ì— í•™ìŠµì„ timestep $[1,\dots,T]$ì´ ì•„ë‹Œ subset $[\tau_1,\dots,\tau_S]$ì— ëŒ€í•´ í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ê³  í•´ì„œ ì•ì„œì„œ ë§í•œ objective functionì´ ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## 4. ë‚˜ì˜ ìƒê°

ğŸ’­ Sampling ì†ë„ë¥¼ í–¥ìƒ ì‹œí‚¨ ê²ƒë„ í° ì—­í• ì„ í–ˆì§€ë§Œ diffusion modelì„ deterministicí•˜ê²Œ ë§Œë“¬ìœ¼ë¡œì¨ latentê°€ ê³ ì •ë˜ëŠ” ê²ƒì´ ê°€ì¥ í° ì¥ì ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ GANsì²˜ëŸ¼ latentë¥¼ í™œìš©í•´ ì˜ë¯¸ìˆëŠ” style transferê°€ ê°€ëŠ¥í•œ ê²ƒì´ ê°€ì¥ í° ì¥ì ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ’­ ë˜í•œ Gaussianì´ ì•„ë‹Œ ë‹¤ë¥¸ ë¶„í¬ì— ëŒ€í•´ì„œë„ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒë„ í•˜ë‚˜ì˜ ì¥ì ì´ë¼ê³  ìƒê°ì€ í•˜ì§€ë§Œ ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ì„ ë•Œ Gaussian distributionì´ë¯€ë¡œ í° ì¥ì ì´ë¼ê³ ëŠ” í•  ìˆ˜ ì—†ì§€ ì•Šì„ê¹Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë…¼ë¬¸ì—ì„œëŠ” ì´ì ì„ ê°•ì¡°í•˜ê³  ìˆì–´ì„œ ì•ìœ¼ë¡œ Gaussianì´ ì•„ë‹Œ ê²½ìš°ë¥¼ ë‹¤ë£¨ëŠ” ë…¼ë¬¸ì´ ìˆëŠ”ì§€ ì£¼ì˜ê¹Šê²Œ ì‚´í´ë´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
