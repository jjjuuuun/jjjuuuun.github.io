---
layout: post
title: "[Generative Model] DDPM : Denoising Diffusion Probabilistic Models"
author: kjy
date: 2024-01-03 16:00:00 +09:00
categories: [Deep Learning, Paper Reading]
tags: [Deep Learning, Paper Reading, Computer Vision, Generative Model]
comments: true
toc: true
math: true
---

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [DDPM : Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) ë…¼ë¬¸ì„ í•¨ê»˜ ì½ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¬¸ì¥ë§ˆë‹¤ í•´ì„ì„ í•˜ê¸° ë³´ë‹¤ëŠ” ê° ë¬¸ë‹¨ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ìš”ì•½í•˜ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

## Abstract

![](../../assets/img/Paper_Reading/DDPM/ddpm_1.jpg){: width="400" .left}

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì€ diffusion probability modelsì„ ì‚¬ìš©í•´ ë†’ì€ í€„ë¦¬í‹°ì˜ ì´ë¯¸ì§€ë¥¼ í•©ì„±í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ğŸ’¡ [Nonequilibrium thermodynamics](https://ko.wikipedia.org/wiki/%EB%B9%84%ED%8F%89%ED%98%95%EC%97%B4%EC%97%AD%ED%95%99)

â“ Diffusion probabilistic modelsì™€ denoising score matchingì˜ ìƒˆë¡œìš´ ì—°ê²°

â“ Generalization of autoregressive decoding

![](../../assets/img/Paper_Reading/blank.png){: .normal}

## 1. Introduction

![](../../assets/img/Paper_Reading/DDPM/ddpm_2.jpg){: width="400" .left}

ğŸ” Generative models : `GANs`, `Autoregressive models`, `flows`, `VAEs`, `Energy-based modeling`, `Score matching`

![](../../assets/img/Paper_Reading/blank.png){: .normal}

![](../../assets/img/Paper_Reading/DDPM/ddpm_3.jpg){: width="400" .left}

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” diffusion probabilistic models(diffusion model)ì˜ progressë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

ğŸ” Diffusion modelì€ ìœ í•œí•œ ì‹œê°„ì´í›„ì— dataì™€ ì¼ì¹˜í•˜ëŠ” samplesì„ ìƒì„±í•˜ê¸° ìœ„í•´ [`variational inference`](#231-variational-inference)ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ `parameterized Markov chain`ì…ë‹ˆë‹¤.

ğŸ’¡ Parameterized Markov chain : Markov propertyë¥¼ ê°€ì§„ ì´ì‚°í™•ë¥ ê³¼ì •

ğŸ’¡ Markov property : íŠ¹ì • ìƒíƒœì˜ í™•ë¥ ì€ ì˜¤ì§ ê³¼ê±°ì˜ ìƒíƒœì— ì˜ì¡´

ğŸ” `Reverse a diffusion process`ë¥¼ í†µí•´ í•™ìŠµ : <u>ì›ë˜ì˜ signal(image)ì´ íŒŒê´´ë  ë•Œê¹Œì§€ samplingì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ dataì— noiseë¥¼ ì¶”ê°€í•˜ëŠ” markov chain</u>ì„ í†µí•´ í•™ìŠµ

â“ Samplingì˜ ë°©í–¥?

![](../../assets/img/Paper_Reading/blank.png){: .normal}

![](../../assets/img/Paper_Reading/DDPM/ddpm_4.jpg){: width="400" .left}

ğŸ” ê¸°ì¡´ ì—°êµ¬ : Diffusion modelì´ ì •ì˜í•˜ê³  í•™ìŠµí•˜ê¸°ì—ëŠ” ìˆ˜ì›”í•˜ì§€ë§Œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤.

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼

1ï¸âƒ£ Diffusion modelì˜ íŠ¹ì • ë§¤ê°œë³€ìˆ˜í™”ê°€ í›ˆë ¨ ì¤‘ multiple noise levelì— ëŒ€í•œ denoising score matchingê³¼ sampling ì¤‘ annealed Langevin dynamicsì™€ ë“±ê°€ì„ì„ ë°œê²¬

2ï¸âƒ£ íŠ¹ì • ë§¤ê°œë³€ìˆ˜í™”ë¥¼ í†µí•´ ì–»ì€ ì¢‹ì€ í’ˆì§ˆì˜ ê²°ê³¼

3ï¸âƒ£ Log likelihoods : Energy based models & score matching < Diffusion model < Other likelihood-based models

4ï¸âƒ£ Diffusion modelì˜ ë§ì€ lossless codelengthsê°€ ì´ë¯¸ì§€ ì„¸ë¶€ ì •ë³´ë¥¼ ì„¤ëª…í•˜ëŠ”ë° ì‚¬ìš©

5ï¸âƒ£ Sampling procedure of diffusion models : Type of progressive decoding

â“ ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ ë¶€ë¶„ì´ ìˆìœ¼ë‚˜ ë’¤ì— ì„¤ëª…ì´ ìˆì„ ê²ƒ ê°™ì•„ ì¼ë‹¨ì€ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.

![](../../assets/img/Paper_Reading/blank.png){: .normal}

## 2. Background

### 2.1 Reverse Process

![](../../assets/img/Paper_Reading/DDPM/ddpm_5.jpg){: .normal}

ğŸ” Reverse Process : Standard normal distributionì—ì„œ í•™ìŠµ dataë¡œ denoising í•´ê°€ëŠ” ê³¼ì •ì´ë©° í•™ìŠµì˜ ëŒ€ìƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}
p_\theta(\mathbf{x}_{0:T})
&= p_\theta(\mathbf{x}_0|\mathbf{x}_{1:T})\cdot p_\theta(\mathbf{x}_{1:T}) \tag{1} \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_{1:T})\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_{2:T})\cdot p_\theta(\mathbf{x}_{2:T}) \tag{2} \\
& \qquad \vdots \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_{1:T})\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_{2:T})\cdots p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)\cdot P_\theta(\mathbf{x}_T) \tag{3} \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_1)\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_2)\cdots p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)\cdot p_\theta(\mathbf{x}_T) \tag{4} \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_1)\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_2)\cdots p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)\cdot p(\mathbf{x}_T) \tag{5} \\
&= p(\mathbf{x}_T)\cdot \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) \tag{6}
\end{align}
$$

#### 2.1.1 Formula (1), (2), (3)

ë¨¼ì € í‘œê¸°ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ë©´ $p_\theta(\mathbf{x}\_{0:T})$ëŠ” ë§¤ê°œë³€ìˆ˜ $\theta, \mathbf{x}\_0, \mathbf{x}\_1, \cdots, \mathbf{x}\_T$ì˜ ê²°í•©í™•ë¥ ì—ì„œ $\theta$ë¥¼ ê³ ì •ì‹œì¼°ì„ ë•Œì˜ ì£¼ë³€í™•ë¥ ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì— ê¸°ìˆ ë˜ì–´ ìˆëŠ” ê³µì‹(chain rule)ì„ ì‚¬ìš©í•˜ë©´ `(1)`, `(2)`, `(3)`ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align} P(A, B, C) &= P(A \cap B \cap C) \\
&= P(A \cap (B \cap C)) \\
&= P(A | B \cap C)P(B \cap C) \\
&= P(A | B, C)P(B, C)
\end{align}
$$

#### 2.1.2 Formula (4)

Reverse processëŠ” markov propertyë¥¼ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì— markov processì— ì˜í•´ì„œ `(4)`ì²˜ëŸ¼ `(3)`ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 2.1.3 Formula (5)

Timestep $T$ì—ì„œì˜ $\mathbf{x}_T$ëŠ” $\theta$ì™€ ìƒê´€ì—†ì´ $\mathcal{N}(0, I)$ì´ê¸° ë•Œë¬¸ì— $\theta$ë¥¼ ì§€ì›Œ `(5)`ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 2.1.4 Formula (6)

`(5)`ë¥¼ ì¼ë°˜í™” ì‹œí‚¤ë©´ `(6)`ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.2 Forward Process (Diffusion Process)

![](../../assets/img/Paper_Reading/DDPM/ddpm_6.jpg){: .normal}

ğŸ” Forward Process(Diffusion process) : Dataì—ì„œ noiseë¥¼ ì¶”ê°€í•´ standard normal distributionìœ¼ë¡œ ê°€ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

$$
\begin{align}
q(\mathbf{x}_{1:T}|\mathbf{x}_0)
&= q(\mathbf{x}_{2:T}|\mathbf{x}_0, \mathbf{x}_1)\cdot q(\mathbf{x}_1|\mathbf{x}_0) \tag{1}\\
&= q(\mathbf{x}_{3:T}|\mathbf{x}_{0:2})\cdot q(\mathbf{x}_2|\mathbf{x}_0, \mathbf{x}_1)\cdot q(\mathbf{x}_1|\mathbf{x}_0) \tag{2}\\
& \qquad \vdots \\
&= q(\mathbf{x}_T|\mathbf{x}_{0:T-1})\cdot q(\mathbf{x}_{T-1}|\mathbf{x}_{0:T-2})\cdots q(\mathbf{x}_1|\mathbf{x}_0) \tag{3}\\
&= q(\mathbf{x}_T|\mathbf{x}_{T-1})\cdot q(\mathbf{x}_{T-1}|\mathbf{x}_{T-2})\cdots q(\mathbf{x}_1|\mathbf{x}_0) \tag{4}\\
&= \prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1}) \tag{5}
\end{align}
$$

#### 2.2.1 Formula (1), (2), (3)

ì•„ë˜ì— ê¸°ìˆ ë˜ì–´ ìˆëŠ” ê³µì‹(chain rule)ì„ ì‚¬ìš©í•˜ë©´ `(1)`, `(2)`, `(3)`ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}
P(A, B, C | D)
&= P(A \cap B \cap C | D) \\
&= \frac{P(A \cap B \cap C \cap D)}{P(D)} \\
&= P(A \cap B | C \cap D)\cdot \frac{P(C \cap D)}{P(D)} \\
&= P(A, B | C, D)\cdot P(C | D) \\
\end{align}
$$

#### 2.2.2 Formula (4)

Forward processëŠ” markov propertyë¥¼ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì— markov processì— ì˜í•´ì„œ `(4)`ì²˜ëŸ¼ `(3)`ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 2.2.3 Formula (5)

`(4)`ë¥¼ ì¼ë°˜í™” ì‹œí‚¤ë©´ `(5)`ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.3 Optimization Function

![](../../assets/img/Paper_Reading/DDPM/ddpm_7.jpg){: .normal}

ğŸ” NLL(Negative Log Likelihood)ì˜ variational boundë¥¼ ì‚¬ìš©í•´ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ğŸ’¡ Log Likelihoodë¥¼ Negative Log Likelihoodë¡œ ë³€ê²½í•œ ì´ìœ  : ìš°ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë¬¸ì œë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¡œ ë°”ê¾¸ê¸° ìœ„í•´

#### 2.3.1 Variational Inference

1. Variational transform

   - ë¹„ì„ í˜• í•¨ìˆ˜ $f(x)$ë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ ì„ í˜• í•¨ìˆ˜ $g(x)$ë¡œ ë°”ê¾¸ëŠ” ê²ƒì„ variational transformì´ë¼ í•©ë‹ˆë‹¤.
   - Variational transformì˜ í•œ ì˜ˆë¡œ $f(x) = \log$í•¨ìˆ˜ë¥¼ ì¼ì°¨í•¨ìˆ˜ë¡œ ë³€ê²½í•´ë³´ê² ìŠµë‹ˆë‹¤.
     > - $\log$ í•¨ìˆ˜ë¥¼ ì¼ì°¨í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ê²ƒì€ $\log$í•¨ìˆ˜ ìœ„ì— ìˆëŠ” ëª¨ë“  ì ì„ ì¼ì°¨í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ëœ»í•©ë‹ˆë‹¤.
     > - $\log$ í•¨ìˆ˜ì˜ ì ‘ì„ ì€ $g(x) = \lambda x + b(\lambda)$ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
     > - ì´ ë•Œ ì¼ì°¨í•¨ìˆ˜ì¸ ì ‘ì„ ì˜ ì ˆí¸ì€ ê¸°ìš¸ê¸° $\lambda$ì— ë”°ë¼ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— $b(\lambda)$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.
     > - ì ‘ì ì„ ì°¾ëŠ” ê³µì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     >
     > $$\begin{align}y = \underset{x}{\text{min}} \{g(x) - f(x)\}\end{align}$$
     >
     > - ì´ì œ ì ‘ì  êµ¬í•´ë³´ê² ìŠµë‹ˆë‹¤.(ê°„ë‹¨í•˜ê²Œ $\log$í•¨ìˆ˜ì´ ë°‘ì´ $10$ì´ë¼ ê°€ì •)
     >
     > $$\begin{align} \frac{d}{dx} \{ g(x) - f(x) \} &=& 0 \\ \frac{d}{dx} \{ \lambda x + b(\lambda) - \ln\ x\} &=& 0 \\ \lambda - \frac{1}{x} &=& 0 \\ x &=& \frac{1}{\lambda}  \end{align}$$
     >
     > - ì •ë¦¬ë¥¼ í•´ë³´ë©´ $x = \frac{1}{\lambda}$ì¸ ëª¨ë“  $x$ì¢Œí‘œì—ì„œ $\log$ í•¨ìˆ˜ë¥¼ ì¼ì°¨í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - ê·¸ëŸ¬ë‚˜ variationa transformì—ì„œ ì¤‘ìš”í•œ íŠ¹ì§•ì´ í•˜ë‚˜ ìˆìŠµë‹ˆë‹¤.
   - ê·¸ê²ƒì€ ë°”ë¡œ í•¨ìˆ˜ê°€ convexí•˜ê±°ë‚˜ concaveí•´ì•¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
     > ë§Œì•½ ë¹„ì„ í˜• í•¨ìˆ˜ $f(x)$ê°€ convexí•˜ì§€ ì•Šê³  concaveí•˜ì§€ë„ ì•Šë‹¤ë©´ ì ‘ì„  $g(x)$ì—ëŠ” ë‘ ê°œ ì´ìƒì˜ ì ‘ì ì´ ìƒê¸°ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
   - ì´ëŸ¬í•œ íŠ¹ì§• ë•Œë¬¸ì— dualityë¥¼ í†µí•´ variational tranformì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
     > - ë§Œì•½ convexí•˜ì§€ ì•Šê±°ë‚˜ concaveí•˜ì§€ ì•Šì€ í•¨ìˆ˜ê°€ ìˆë‹¤ë©´ $\log$ë¥¼ ì‚¬ìš©í•´ concaveí•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     > - Deep Learningì„ ê³µë¶€í•˜ë‹¤ë³´ë©´ $\log$ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì—ëŠ” ê³±ì„ í•©ìœ¼ë¡œ ë³€ê²½í•´ì£¼ëŠ” ì´ìœ ë„ ìˆì§€ë§Œ concaveí•œ íŠ¹ì§•ì„ ì‚¬ìš©í•´ variational transformí•˜ëŠ” ì´ìœ ë„ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

2. Variational Inference

   - Variational inferenceëŠ” variational transformì„ ì‚¬ìš©í•´ Posteroir distribution $p(\theta\|x)$ë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ ë§¤ê°œë³€ìˆ˜ $\theta$ë¥¼ ì¡°ì •í•˜ì—¬ í™•ë¥ ë¶„í¬ $q(\theta)$ë¡œ approximation í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.

#### 2.3.2 Variational Inference at DDPM

DDPMì—ì„œ variational inferenceì„ ì‚¬ìš©í•˜ëŠ” ê³µì‹ì„ í†µí•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

$$
\begin{align}
\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p_\theta(\mathbf{x}_0)\right]

&= \int(-\log\ p_\theta(\mathbf{x}_0))\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \because \text{Monte Carlo Integration} \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)}) \cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \because \text{Chain Rule} \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)} \cdot \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \cdot \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T + \int(-\log\ \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T - \int(\log\ \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T - D_{KL}(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\ ||\ p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)) \qquad & \because q\text{ë¥¼ í†µí•´ } p_\theta\text{ë¥¼ ì°¾ìœ¼ë ¤ê³  í•¨}\ (\text{Variational Inference}) \\

&\le \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T\ \qquad & \because D_{KL} \ge 0 \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \qquad & \\

&= ELBO \qquad & \\
\end{align}
$$

#### 2.3.3 Optimization Function êµ¬í•˜ê¸°

[Section 2.1](#21-reverse-process)ê³¼ [section 2.2](#22-forward-process-diffusion-process)ë¥¼ ì‚¬ìš©í•´ ìœ„ì—ì„œ variational inferenceë¥¼ í†µí•´ ì°¾ì€ ELBO termì„ ì•„ë˜ì™€ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}
\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p_\theta(\mathbf{x}_0)\right]

&\le \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{p(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{p(\mathbf{x}_T)\cdot \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{\prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \qquad & \because \text{Section 2.1}\ \&\ \text{Section2.2} \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\log\prod_{t=1}^T \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\left( \log\frac{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_0)} + \log\frac{p_\theta(\mathbf{x}_1|\mathbf{x}_2)}{q(\mathbf{x}_2|\mathbf{x}_1)} + \cdots + \log\frac{p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_{T-1})} \right)\right] \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=1}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \qquad & \\

&=: L \qquad & \\

\end{align}
$$

ì§€ê¸ˆê¹Œì§€ ë§ì€ ê³µì‹ë“¤ì´ ë‚˜ì™€ í—·ê°ˆë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ì§€ê¸ˆê¹Œì§€ ì¦ëª…í–ˆë˜ ê³µì‹ì„ ê°„ë‹¨íˆ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p_\theta(\mathbf{x}_0)\right] \le \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{p(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] = \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=1}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] =: L$$

### 2.4 Variance schedule $\beta\_t$

...ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.
