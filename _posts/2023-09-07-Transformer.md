---
layout: post
title: "[Machine Translation] Attention Is All You Need(Transformer)"
author: kjy
date: 2023-09-07 13:23:00 +09:00
categories: [Paper Review, NLP]
tags: [Paper Review, NLP, Machine Translation]
comments: true
toc: true
math: true
---

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [Attention Is All You Need(Transformer)](https://arxiv.org/pdf/1706.03762.pdf) ë…¼ë¬¸ ë¦¬ë·°ë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

---

## Abstract

í•´ë‹¹ ë…¼ë¬¸ì€ ì´ì „ ì—°êµ¬ë“¤ê³¼ ë‹¬ë¦¬ ì˜¤ì§ attentionì´ë¼ëŠ” ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ machine translation taskë¥¼ ìˆ˜í–‰í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ì˜¤ì§ attention ë©”ì»¤ë‹ˆì¦˜ë§Œì„ ì‚¬ìš©í•œ ëª¨ë¸ì„ transformerë¼ê³  í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ë¶€ë¥´ëŠ”ë° ì´ëŸ¬í•œ transformer ëª¨ë¸ì´ ë‘ ê°€ì§€ machine translationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ê³¼ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ìì„¸í•œ í•™ìŠµ ë°©ë²•ë³´ë‹¤ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë“¤ì–´ê°€ëŠ”ì§€ ì‚´í…ë³´ê² ìŠµë‹ˆë‹¤. ìì„¸í•œ í•™ìŠµ hyperparameterê°€ ê¶ê¸ˆí•˜ì‹  ë¶„ë“¤ê»˜ì„œëŠ” ë…¼ë¬¸ì„ ì§ì ‘ ì°¸ê³ í•´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

## Transformer Model êµ¬ì¡°

Transformer modelì˜ êµ¬ì¡°ë¥¼ í•˜ë‚˜ì”© ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

### Encoder-Decoder Stacks

Transformer modelì˜ í° êµ¬ì¡°ëŠ” encoder-decoder êµ¬ì¡°ì…ë‹ˆë‹¤. Encoder Decoder êµ¬ì¡°ë¥¼ ì²˜ìŒ machine translationì— ê°€ì ¸ì˜¨ [Seq2Seq](https://jjjuuuun.github.io/posts/Seq2Seq/)ì—ì„œì˜ êµ¬ì¡°ì™€ ë‹¬ë¦¬ encoder(decoder) layerë¥¼ ì—¬ëŸ¬ê°œ ìŒ“ì•„ í•˜ë‚˜ì˜ encoder(decoder) blockì„ ë§Œë“­ë‹ˆë‹¤.

TransformerëŠ” ë™ì¼í•œ êµ¬ì¡°ì˜ encoder layer 6ê°œì™€ ë™ì¼í•œ êµ¬ì¡°ì˜ decoder layer 6ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ì´ë¥¼ ê°„ë‹¨íˆ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_1.png){: class="align-center"}

ğŸ¤” ìœ„ì˜ ê·¸ë¦¼ì—ì„œ encoderì—ì„œ decoder ë°©í–¥ìœ¼ë¡œ í™”ì‚´í‘œê°€ ê·¸ë ¤ì ¸ ìˆëŠ”ë° ì´ê²ƒì€ ë’¤ì—ì„œ ì„¤ëª…ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ìš°ì„  í° í‹€ì—ì„œ encoderì™€ decoderì˜ êµ¬ì¡°ê°€ ì €ë ‡ê²Œ ìƒê²¼ë‹¤ëŠ” ê²ƒë§Œ ì•Œê³  ë„˜ì–´ê°€ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

### Data

í•´ë‹¹ ëª¨ë¸ì—ì„œëŠ” ë‘ ê°€ì§€ì˜ datasetsì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ê° datasetsì˜ êµ¬ì„±ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- WMT 2014 English-German dataset

  - 450ë§Œ ê°œì˜ ë¬¸ì¥
  - [BPE(Byte-Pair Encoding)](https://jjjuuuun.github.io/posts/BPE/) subtokenizerë¥¼ ì‚¬ìš©í•´ 32,000ê°œì˜ tokenìœ¼ë¡œ datasetì„ êµ¬ì„±

- WMT 2014 English-French dataset

  - 3600ë§Œ ê°œì˜ ë¬¸ì¥
  - [WPM(WordPiece Model)](https://jjjuuuun.github.io/posts/WordPiece/) subtokenizerë¥¼ ì‚¬ìš©í•´ 37,000ê°œì˜ tokenìœ¼ë¡œ datasetì„ êµ¬ì„±

Transformer modelì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„œ `Attention Is All You Need`ë¼ëŠ” ì˜ì–´ ë¬¸ì¥ì„ `Aufmerksamkeit ist alles, was Sie brauchen`ë¼ëŠ” ë…ì¼ì–´ ë¬¸ì¥ìœ¼ë¡œ ê¸°ê³„ ë²ˆì—­ì„ í•˜ëŠ” ìƒí™©ì„ ì˜ˆì‹œë¡œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ë˜í•œ ìœ„ì—ì„œ ë§ì”€ë“œë¦° tokenizerë¥¼ ì§ì ‘ ì ìš©í•˜ì—¬ ì„¤ëª…í•˜ê¸°ì—ëŠ” ë¬´ë¦¬ê°€ ìˆì–´ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ pre-tokenizeí–ˆë‹¤ëŠ” ê°€ì •í•˜ì— ì„¤ëª…ì„ ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

Datasetsì„ ì¤€ë¹„í•˜ê³  ì˜ˆì‹œ ë¬¸ì¥ì„ tokenizeí•œ ê²°ê³¼ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ $n$ê³¼ $n'$ì€ ê°ê° encoderì™€ decoderì˜ maximum sequence length ì…ë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_2.png){: class="align-center"}

ğŸ” ìœ„ì˜ ì‚¬ì§„ì—ì„œ <u>Decoder Input</u>ì˜ ê²½ìš° ì˜¤ë¥¸ìª½ì—ì„œ í•˜ë‚˜ì”© tokenì´ ì´ë™ë˜ì–´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë° ë…¼ë¬¸ì—ì„œëŠ” ì´ê²ƒì„ <u>shifted right</u>ë¼ê³  í‘œí˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### Embedding

Transformerì—ì„œëŠ” $d_{model} = 512$ìœ¼ë¡œ embeddingì„ ì§„í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. $512$ì°¨ì›ì„ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë¦¬ê¸°ì—ëŠ” ë¬´ë¦¬ê°€ ìˆìœ¼ë¯€ë¡œ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_3.png){: class="align-center"}

ğŸ” Embeddingì„ ì§„í–‰í•˜ëŠ” ê³¼ì •ì€ encoderì™€ decoder ëª¨ë‘ ê°™ìŠµë‹ˆë‹¤.
ğŸ” Embedding layerì— $\sqrt{d_{model}}$ì„ ê³±í•©ë‹ˆë‹¤.

### Positional Encoding

ìœ„ì—ì„œ ë§ì”€ë“œë¦° ê²ƒì²˜ëŸ¼ transformerëŠ” ì˜¤ì§ attention ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œë§Œ êµ¬ì„±í•œ ì²« ëª¨ë¸ì…ë‹ˆë‹¤. Transformerë¥¼ ì´ë ‡ê²Œ êµ¬ì„±í•˜ë©´ì„œ ë‹¨ì–´ë“¤ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë¸ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ë°©ë²•ì„ ì €ìë“¤ì€ ê³ ì•ˆí–ˆì–´ì•¼ í–ˆìŠµë‹ˆë‹¤.

ì´ë ‡ê²Œ ë‹¨ì–´ë“¤ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë¸ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” positional embeddingê³¼ positional encodingì´ ìˆìŠµë‹ˆë‹¤. Positional embedding ê°™ì€ ê²½ìš°ì—ëŠ” í•™ìŠµì´ ê°€ëŠ¥í•˜ì§€ë§Œ ê·¸ë§Œí¼ weightê°€ ëŠ˜ì–´ë‚˜ëŠ” ë¬¸ì œì™€ trainingì—ì„œ ë§Œë‚˜ì§€ ëª»í•œ ê¸¸ì´ì˜ ë¬¸ì¥ì´ inferenceí•  ë•Œ ë“¤ì–´ì˜¨ë‹¤ë©´ ëŒ€ì‘ì„ í•˜ì§€ ëª»í•˜ëŠ” ë‹¨ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ positional encoding ê°™ì€ ê²½ìš°ì—ëŠ” í•™ìŠµì„ í•˜ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ trainingì—ì„œë³´ë‹¤ ê¸´ ë¬¸ì¥ì´ inferenceí•  ë•Œ ë“¤ì–´ì˜¤ë”ë¼ë„ ëŒ€ì‘ì´ ì¶©ë¶„íˆ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì´ë ‡ê²Œë§Œ ìƒê°ì„ í•´ë³´ë©´ ë¬´ì¡°ê±´ positional encodingì„ ì‚¬ìš©í•´ì•¼ í•  ê²ƒ ê°™ì§€ë§Œ ë§Œì•½ ì„±ëŠ¥ì ì¸ ì¸¡ë©´ì—ì„œ positional embeddingì´ ì›”ë“±íˆ ì¢‹ë‹¤ë©´ psotional embeddingì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì‹¤í—˜ì„ í•´ ë³¸ ê²°ê³¼ ë‘ ë°©ë²•ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ ë³„ë¡œ ì—†ì–´ì„œ ì €ìë“¤ì€ positional encodingì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

Positional encodingì— ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë¡œ ì €ìë“¤ì€ sinusoidë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

$$
PE_{pos, i} = \begin{cases} sin(\frac{pos}{10000^{i/d_{model}}}), &  \mbox{if }i\mbox{ is even} \\ cos(\frac{pos}{10000^{i/d_{model}}}), & \mbox{if }i\mbox{ is odd}\end{cases}
$$

ğŸ” SinusoidëŠ” ì£¼ê¸°í•¨ìˆ˜ì¸ë° ì €ìë“¤ì´ ì£¼ê¸°í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì´ìœ ëŠ” $PE_{pos+k}$ë¥¼ $PE_{pos}$ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ì˜ ë°°ìš¸ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì´ë¼ê³  ë§í•©ë‹ˆë‹¤.

ë˜í•œ ê³ ì •ëœ ì°¨ì›($d_{model}$)ë¡œ encodingì„ í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì„ embddingí•œ ê²°ê³¼ì— ë”í•˜ì—¬ ëª¨ë¸ì—ê²Œ inputìœ¼ë¡œ ë„£ì–´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_4.png){: class="align-center"}

ğŸ” Positional encodingì„ ì§„í–‰í•˜ëŠ” ê³¼ì •ì€ encoderì™€ decoder ëª¨ë‘ ê°™ìŠµë‹ˆë‹¤.

### Multi-Head Attention

#### Query, Key, Value

Query, key, valueë¥¼ ë¬¶ì–´ì„œ ìƒê°í•´ë³´ë©´ DataBase ë¶„ì•¼ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ìš©ì–´ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì €ëŠ” DBì™€ ì—°ê´€ì§€ì–´ì„œ transformerì—ì„œ ì‚¬ìš©ë˜ëŠ” query, key, valueë¥¼ ì´í•´í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

|       | DB                                                       | Transformer                                              |
| :---- | :------------------------------------------------------- | :------------------------------------------------------- |
| Query | DBì— íŠ¹ì • ì •ë³´ë¥¼ ìš”ì²­                                    | í•´ë‹¹ ë‹¨ì–´ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ë©´ ë ê¹Œìš”?                      |
| Key   | DBì— ì €ì¥ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ìœ ì¼í•˜ê²Œ êµ¬ë³„í•  ìˆ˜ ìˆëŠ” ì‹ë³„ì | ìš”ì²­ëœ(Query) ë‹¨ì–´ì™€ ê´€ë ¨ìˆëŠ” ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ì‹ë³„ì |
| Value | DBì— ì‹¤ì œ ì €ì¥ë˜ì–´ ìˆëŠ” ì •ë³´                             | ì‹¤ì œ ê´€ë ¨ì´ ìˆëŠ” ë‹¨ì–´                                    |

Transformerì—ì„œ query, key, valueëŠ” linear projectionì„ í†µí•´ êµ¬í•©ë‹ˆë‹¤. ë¬¼ë¡  linear projectionì„ í•˜ì§€ ì•Šê³  **embdding + positional encoding**ì„ í•œ ê²°ê³¼ë¥¼ ë°”ë¡œ query, key, valueë¡œ ì‚¬ìš©ê°€ëŠ¥í•˜ì§€ë§Œ ì €ìë“¤ì€ query, key, valueë¥¼ ê°ê° $d_k, d_k, d_v$ë¡œ linear projectioní•˜ëŠ” ê²ƒì´ ìœ ìµí•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

Query, Key, Valueë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_5.png){: class="align-center"}

#### SDPA(Scaled Dot-Product Attention)

Attentionì„ êµ¬í•¨ì— ìˆì–´ì„œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 2ê°€ì§€ ì •ë„ê°€ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ additive attentionê³¼ dot-product attentionì…ë‹ˆë‹¤.

ì´ ë‘ ë°©ë²•ì€ ë³µì¡ì„± ì¸¡ë©´ì—ì„œëŠ” ìœ ì‚¬í•˜ì§€ë§Œ `numpy`ì™€ ê°™ì€ matrix multiplication codeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— dot-product attention functionì´ í›¨ì”¬ ë¹ ë¥´ê³  ê³µê°„ì´ íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì— transformerì—ì„œëŠ” dot-product attentionì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ dot-product attention functionì— $\sqrt{d_k}$ë¡œ scalingì„ í•˜ê²Œ ë˜ëŠ”ë° ê·¸ ì´ìœ ëŠ” $d_k$ê°€ ë„ˆë¬´ í° ê²½ìš° ì´í›„ $\text{softmax}$ì˜ gradientë¥¼ êµ‰ì¥íˆ ì‘ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì— dot-product attentionë³´ë‹¤ additive attentionì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§€ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤.

ìœ„ì— ë‚´ìš©ë“¤ì„ ì¢…í•©í–ˆì„ ë•Œ transformerì—ì„œ ì‚¬ìš©ë˜ëŠ” attentionì€ scaled dot-product attentionì´ê³  êµ¬í•˜ëŠ” ê³µì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $$

ì´ ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_6.png){: class="align-center"}

ë˜í•œ í•´ë‹¹ íŒŒíŠ¸ì—ì„œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ ì´ìœ ë¡œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.

- Encoder
  - ê¸¸ì´ê°€ ì§§ì€ ë¬¸ì¥ì—ì„œ ë‚¨ì€ ê¸¸ì´ì— ëŒ€í•œ ì²˜ë¦¬
  - ì‚¬ìš©í•˜ì§€ ì•Šì€ token ì²˜ë¦¬
- Decoder
  - ê¸¸ì´ê°€ ì§§ì€ ë¬¸ì¥ì—ì„œ ë‚¨ì€ ê¸¸ì´ì— ëŒ€í•œ ì²˜ë¦¬
  - ì‚¬ìš©í•˜ì§€ ì•Šì€ token ì²˜ë¦¬
  - ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ ì´ì „ ë‹¨ì–´ë“¤ê³¼ í˜„ì¬ ë‹¨ì–´ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ ì²˜ë¦¬

ë§Œì•½ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ë¥¼ í•˜ê²Œ ëœë‹¤ë©´ SDPA ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìœ¼ë©° ì•„ë˜ì˜ ê·¸ë¦¼ì€ decoderì—ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ í˜„ì¬ ë‹¨ì–´ ê¸°ì¤€ ì˜¤ë¥¸ìª½ ë‹¨ì–´ë“¤ì„ ëª¨ë‘ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•œ ê²½ìš°ì…ë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_7.png){: class="align-center"}

ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ $\text{softmax}$ê°’ì„ ì²˜ë¦¬í•˜ê¸° ì „ì— í•˜ê²Œ ë˜ëŠ”ë° ì´ë•Œ ë§ˆìŠ¤í‚¹ ëœ ë¶€ë¶„ì„ $-\infty$ë¡œ í•˜ê²Œ ë˜ë©´ $\text{softmax}$ì˜ ê°’ì´ 0ì´ ë˜ë¯€ë¡œ ëª¨ë¸ì´ ë¬´ì‹œí•˜ê²Œ ë©ë‹ˆë‹¤.

ìœ„ì˜ ê³¼ì •ë“¤ì„ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_8.png){: class="align-center"}

#### Multi-Head Attention

SDPAì˜ ê²°ê³¼ë¥¼ Single-Head attentionì´ë¼í•˜ë©° ì—¬ëŸ¬ ê°œì˜ SDPAë¥¼ ì¡°í•©í•œ ê²ƒì´ Multi-Head Attention ì…ë‹ˆë‹¤.

Multi-Head Attentionì€ $h$ê°œì˜ SDPA outputì„ concatenateí•œ í›„ linear projectioní•´ì„œ $d_{model}$ì°¨ì›ì˜ outputì„ ì–»ìŠµë‹ˆë‹¤. ë˜í•œ ê³„ì‚° ë¹„ìš©ì„ Single-Head Attentionê³¼ ìœ ì‚¬í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ $d_k = d_v = d_{model}/h = 64 (h = 8)$ë¡œ hyperparameterë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_9.png){: class="align-center"}

### FFN(Feed Forward Network)

Encoder layerì™€ decoder layer ëª¨ë‘ì—ì„œ ë§ˆì§€ë§‰ Multi-Head Attentionì˜ outputì„ FFNì„ í•œ ë²ˆ ë” í†µê³¼ì‹œí‚¤ëŠ”ë° FFNì˜ ê³µì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$ \text{FFN}(x) = \text{ReLU}(xW_1\ +\ b_1)W_2 + b_2 $$

ì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_10.png){: class="align-center"}

### Encoder Layerì™€ Decoder Layer

![](../../assets/img/transformer/transformer_11.png){: class="align-center"}

#### Encoder Layer

Encoder layerëŠ” Multi-Head attentionê³¼ FFNìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ì¶”ê°€ì ìœ¼ë¡œ residual connection, layer normalization, dropoutì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_12.png){: class="align-center" width="300px" height="500px"}

#### Decoder Layer

Decoder layerëŠ” Masked Multi-Head attentionê³¼ Multi-Head attention(encoder-decoder attention), FFNìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ì¶”ê°€ì ìœ¼ë¡œ residual connection, layer normalization, dropoutì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œ encoder-decoder attentionì´ ë“±ì¥í•˜ëŠ”ë° êµ¬ì¡°ëŠ” Multi-Head attentionê³¼ ê°™ì§€ë§Œ QueryëŠ” decoder layerì˜ ì´ì „ Masked Multi-Head attentionì˜ ê²°ê³¼ë¡œë¶€í„° ì˜¤ê³  Keyì™€ ValueëŠ” Encoder(ë§ˆì§€ë§‰ encoder layer)ì˜ attention ê°’ìœ¼ë¡œë¶€í„° ì˜¨ë‹¤ëŠ” ì ì´ ë‹¤ë¦…ë‹ˆë‹¤. í•´ë‹¹ ë¶€ë¶„ì´ encoderì™€ decoderë¥¼ ì—°ê²°í•˜ëŠ” ë¶€ë¶„ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ” ìœ„ì—ì„œ ì–¸ê¸‰í•˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ Query, key, Valueê°€ ëª¨ë‘ ê°™ì€ ê²°ê³¼ë¡œë¶€í„° ê³„ì‚°ë˜ë©´ self-attentionì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ì¦‰, encoder-decoder attentionì„ ì œì™¸í•œ transformerì—ì„œì˜ ëª¨ë“  attentionì€ self-attentionì´ë¼ê³  ë´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_13.png){: class="align-center" width="300px" height="500px"}

Encoder-Decoder attentionì„ ìì„¸íˆ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![](../../assets/img/transformer/transformer_14.png){: class="align-center"}

## Transformer Modelì˜ ê²°ê³¼

- WMT 2014 English-German dataset
- BLEU Score 28.4ë¡œ SOTA ë‹¬ì„±

- WMT 2014 English-to-French dataset
  - BLEU Score 41.0ìœ¼ë¡œ SOTA ë‹¬ì„±

## Key Point

- TransformerëŠ” recurrent layer(Seq2Seq, LSTM), Convolutional layerì„ ê¸°ë°˜ìœ¼ë¡œ í•œ architectureë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ í›ˆë ¨ì´ ë©ë‹ˆë‹¤.
- Recurrent layer, attentionì„ í•¨ê»˜ ì“°ëŠ” ëª¨ë¸ì—ì„œ attentionë§Œì„ ì‚¬ìš©í•œ ìµœì´ˆì˜ sequence transduction modelì´ transformer ì…ë‹ˆë‹¤.
- WMT 2014 English-to-German and WMT 2014 English-to-Frenchì—ì„œ ëª¨ë‘ SOTA ë‹¬ì„±

## Conclusion

ğŸ¤” í•´ë‹¹ ë…¼ë¬¸ì„ ê³µë¶€í•˜ë©´ì„œ ì–¼í• ì•Œê³  ìˆì—ˆë˜ transformerì˜ êµ¬ì¡°, í•™ìŠµê³¼ì •ì„ ìì„¸íˆ ì•Œ ìˆ˜ ìˆì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ í•´ë‹¹ ë…¼ë¬¸ì´ ë‚˜ì˜¤ê³  í•œì°¸ ë’¤ì— ë…¼ë¬¸ì„ ì½ëŠ” ê²ƒì´ë¼ í•´ë‹¹ ë…¼ë¬¸ì´ í˜„ì¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì–¼ë§ˆë‚˜ í° ì˜í–¥ì„ ë¼ì³¤ëŠ”ì§€ëŠ” ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ëŒ€ë‹¨í•´ ë³´ì´ëŠ” ê²ƒ ê°™ì§€ë§Œ ì½ìœ¼ë©´ì„œ ìƒê°ì˜ ì „í™˜ì´ë¼ëŠ” ê´€ì ì—ì„œ ì •ë§ ëŒ€ë‹¨í•œ ë…¼ë¬¸ì„ì„ ë‹¤ì‹œ ëŠê¼ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ¤” ì´í›„ transformerê°€ ì–´ë–»ê²Œ ë°œì „ë˜ì–´ ê°”ëŠ”ì§€ ê³µë¶€í•´ ê°€ë©´ì„œ transformerì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì¼ í•„ìš”ê°€ ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì•„ì§ ê³µë¶€ë¥¼ ì œëŒ€ë¡œ í•´ë³´ì§„ ëª»í–ˆì§€ë§Œ BERT, GPT, ViT, Swin Transformer, U-Netì— transformer ì‚¬ìš©ê³¼ ê°™ì€ ê²½ìš°ë“¤ì„ ë´¤ê¸° ë•Œë¬¸ì— ì•ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë”°ë¼ê°€ê¸° ìœ„í•´ì„œë¼ë©´ transformer ìì²´ì— ëŒ€í•œ ì‹¬ë„ìˆëŠ” ì´í•´ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.

## â€» Reference

- [https://github.com/hyunwoongko/transformer](https://github.com/hyunwoongko/transformer)
