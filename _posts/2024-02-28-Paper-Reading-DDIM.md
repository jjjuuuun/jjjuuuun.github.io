---
layout: post
title: "[Generative Model] DDIM : Denoising Diffusion Implicit Models"
author: kjy
date: 2024-02-28 22:30:00 +09:00
categories: [Deep Learning, Paper Reading]
tags: [Deep Learning, Paper Reading, Computer Vision, Generative Model]
comments: true
toc: true
math: true
---

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [DDIM : Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) ë…¼ë¬¸ì„ í•¨ê»˜ ì½ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¬¸ì¥ë§ˆë‹¤ í•´ì„ì„ í•˜ê¸° ë³´ë‹¤ëŠ” ê° ë¬¸ë‹¨ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ìš”ì•½í•˜ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

Emojiì˜ ì˜ë¯¸ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

ğŸ” : ë…¼ë¬¸ì— ìˆëŠ” ë‚´ìš©

ğŸ’¡ : ë…¼ë¬¸ì— ì í˜€ìˆì§€ ì•Šì€ ì‚¬ì „ì§€ì‹

ğŸ’­ : ì €ì˜ ìƒê°

â­ : ì¤‘ìš”í•œ ë‚´ìš©

âœ… : ë‚˜ì¤‘ì— í™•ì¸ì´ í•„ìš”í•œ ë‚´ìš©

## Abstract

![](../../assets/img/Paper_Reading/DDIM/ddim_1.jpg){: .normal}

ğŸ” DDPMì˜ ë‹¨ì  : Sampleì„ ìƒì„±í•˜ê¸°ìœ„í•´ ë§ì€ stepì˜ Markov chain ê³¼ì •ì„ ê±°ì³ì•¼ í•©ë‹ˆë‹¤.

ğŸ” DDIM : DDPMê³¼ ê°™ì€ objective functionì„ ë§Œë“œëŠ” non-Markovian diffusion processë¥¼ í†µí•´ í•™ìŠµ

> ğŸ” 10 ~ 50ë°° ì •ë„ DDPMë³´ë‹¤ ë¹ ë¦…ë‹ˆë‹¤.  
> ğŸ” Semantically meaningful imageë¥¼ latent spaceì—ì„œ ë°”ë¡œ interpolationì„ í†µí•´ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> ğŸ” ë§¤ìš° ë‚®ì€ reconstruction errorë¥¼ ê´€ì°°í–ˆìŠµë‹ˆë‹¤.

## 1. Introduction

![](../../assets/img/Paper_Reading/DDIM/ddim_2.jpg){: .normal}

ğŸ” Generative model : GANs, VAE, Autoregressive models, normalizaing flows

ğŸ” ê·¸ ì¤‘ GANsì´ ê°€ì¥ ì¢‹ì€ í’ˆì§ˆì„ ë§Œë“¤ì–´ëƒˆì§€ë§Œ í•™ìŠµ ì•ˆì •í™”ë¥¼ í•˜ê¸° ìœ„í•´ ë§¤ìš° ì‹ ì¤‘í•œ optimizationê³¼ architecture ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤.

ğŸ” ìµœê·¼ì—ëŠ” iterative generative modelsì— ëŒ€í•œ ì—°êµ¬ê°€ ì§„í–‰(DDPM, NCSN)

ğŸ” Iterative generative modelì€ GANsê³¼ ê²¬ì¤„ë§Œí•œ í’ˆì§ˆì„ ìƒì„±í–ˆìœ¼ë©° GANsì— ë¹„í•´ í•™ìŠµì´ ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

ğŸ” ê·¸ëŸ¬ë‚˜ ë†’ì€ í’ˆì§ˆì˜ sampleì„ ìƒì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ë§ì€ iterationì´ í•„ìš”í•´ GANsë³´ë‹¤ ë§¤ìš° ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

![](../../assets/img/Paper_Reading/DDIM/ddim_3.jpg){: .normal}

ğŸ” DDPMê³¼ GANsì˜ gapì„ ì¤„ì´ê¸°ìœ„í•´ DDIMì„ ì œì‹œí•©ë‹ˆë‹¤.

ğŸ” DDIMì€ DDPMì˜ objective functionê³¼ ê°™ì€ objective functionì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ([Section 3](#3-variational-inference-for-non-markovian-forward-process))

ğŸ” Diffusion processì—ì„œ DDPMê³¼ ë‹¬ë¦¬ non-Markovianì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ([Section 4.1](#41-denoising-diffusion-implicit-models))

ğŸ” Reverse processëŠ” ê·¸ëŒ€ë¡œ Markov chainsì„ ì‚¬ìš©í•˜ëŠ”ë° ì§§ì€ Markov chains ì‚¬ìš©í•©ë‹ˆë‹¤. ([Section 4.2](#42-accelerated-generation-processes))

ğŸ” DDPMë³´ë‹¤ ë‚˜ì€ ê²°ê³¼ ([Section 5]())

> 1. ë” ë‚˜ì€ í’ˆì§ˆì˜ sampleì„ ë” ë¹ ë¥¸ ì†ë„ë¡œ ìƒì„±
> 2. DDIMìœ¼ë¡œ ìƒì„±í•œ sampleì€ consistencyë¥¼ ê°€ì§€ê³  ìˆì–´ ë˜‘ê°™ì€ initial latent variableê³¼ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ Markov chainì„ í†µí•´ ìƒì„±ëœ sampleì€ ë¹„ìŠ·í•œ featureë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
> 3. Consistency ë•Œë¬¸ì— initial latent variableì„ ì¡°ì‘í•´ì„œ ì˜ë¯¸ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 2. Background

![](../../assets/img/Paper_Reading/DDIM/ddim_4.jpg){: .normal}

ğŸ’¡ í•´ë‹¹ íŒŒíŠ¸ëŠ” DDPMì— ìˆëŠ” ë‚´ìš©ì„ ë³µìŠµí•˜ëŠ” ë¶€ë¶„ì´ë¯€ë¡œ ë„˜ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

> - [Paper Reading DDPM](https://jjjuuuun.github.io/posts/Paper-Reading-DDPM/)
> - [Paper Review DDPM](https://jjjuuuun.github.io/posts/Paper-Review-DDPM/)

ğŸ’¡ ë‹¤ë§Œ, notationì´ ë‹¤ë¥¸ ë¶€ë¶„ì´ ìˆì–´ ê·¸ ë¶€ë¶„ë§Œ ë§ì”€ë“œë¦¬ê³  ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. (DDIM $\Longrightarrow$ DDPM)

> - $\alpha\_t$ $\longrightarrow$ $\bar{\alpha}\_t$
> - ELBO termì„ ìµœëŒ€í™”í•˜ëŠ” $\theta$ë¥¼ ì°¾ëŠ” ë¬¸ì œ $\longrightarrow$ Negative ELBO termì„ ìµœì†Œí™”í•˜ëŠ” $\theta$ë¥¼ ì°¾ëŠ” ë¬¸ì œ
> - $\gamma\_t$ : ì‹œê°„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜(ê³„ìˆ˜) $\longrightarrow$ $\gamma\_t = 1$

## 3. Variational Inference For Non-Markovian Forward Process

![](../../assets/img/Paper_Reading/DDIM/ddim_5.jpg){: .normal}

ğŸ” Generative modelì€ reverse of inference processì„ approximate

- Inference process : Diffusion process / Forward process
- Reverse of inference process : Denoising process / Reverse process

ğŸ” Generative modelì´ imageë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ iteration ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ inference processë¥¼ ë‹¤ì‹œ ìƒê°í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ğŸ” DDPMì˜ objectiveì¸ $L\_\gamma$ëŠ” marginals $q(\mathbf{x}\_t\|\mathbf{x}\_0)$ì—ë§Œ ì˜ì¡´í•˜ê³  joints $q(\mathbf{x}\_{1:T}\|\mathbf{x}\_0)$ì—ëŠ” ì§ì ‘ ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

$$
\begin{align}
L_\gamma(\epsilon_\theta)
:=& \sum^T_{t=1}\gamma_t\mathbb{E}_{\mathbf{x}_0\sim q(\mathbf{x}_0), \epsilon_T \sim \mathcal{N}(\mathrm{0}, \mathrm{I})}\Big[ \lVert \epsilon^{(t)}_\theta {\color{Red}\mathbf{x}_t}\ -\ \epsilon_t \rVert^2_2 \Big] & \qquad \\
=& \sum^T_{t=1}\gamma_t\mathbb{E}_{\mathbf{x}_0\sim q(\mathbf{x}_0), \epsilon_T \sim \mathcal{N}(\mathrm{0}, \mathrm{I})}\Big[ \lVert \epsilon^{(t)}_\theta (\sqrt{\alpha_t}{\color{Red}\mathbf{x}_0}\ +\ \sqrt{1-\alpha_t}\epsilon_t)\ -\ \epsilon_t \rVert^2_2 \Big] & \qquad \because \mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_0\ +\ \sqrt{1-\alpha_t}\epsilon_t
\end{align}
$$

ğŸ” ê°™ì€ marginals $q(\mathbf{x}\_t\|\mathbf{x}\_0)$ì„ ê°€ì§€ëŠ” joints $q(\mathbf{x}\_{1:T}\|\mathbf{x}\_0)$ê°€ ë§ê¸° ë•Œë¬¸ì— Non-Markovian inference processë¥¼ íƒìƒ‰í•˜ì—¬ ìƒˆë¡œìš´ generative processë¡œ ì´ì–´ì§€ë„ë¡ í•©ë‹ˆë‹¤.

> ğŸ’­ $\mathbf{x}\_t$ë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ” $q(\mathbf{x}\_{1:T}\|\mathbf{x}\_0)$ëŠ” ë§ì€ë° ê·¸ ì¤‘ Non-Markovianì„ ì‚¬ìš©í•´ì„œ íŠ¹ì • $\mathbf{x}\_t$ë¥¼ ë§Œë“¤ë„ë¡ í•´ ìƒˆë¡œìš´ generative processë¼ ì •ì˜í•œë‹¤ëŠ” ì´ì•¼ê¸°ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ” ìƒˆë¡œìš´ generative process ë˜í•œ DDPMê³¼ ë™ì¼í•œ objective functionì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” ë˜í•œ ìƒˆë¡œìš´ generative processë¥¼ ë§Œë“œëŠ” ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ì¸ Non-Markovianì´ Gaussian caseë¥¼ ë„˜ì–´ì„œë„ ì ìš©ë¨ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

### 3.1 Non-Markovian Forward Processes

![](../../assets/img/Paper_Reading/DDIM/ddim_6.jpg){: .normal}

ğŸ” ìš°ì„ , inference processë¥¼ Non-Markovianìœ¼ë¡œ ì •ì˜í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

$$
\begin{align}
& q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_0) := q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod^T_{t=2}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \\

& \qquad \text{where} \\

& \qquad \qquad q_\sigma(\mathbf{x}_T|\mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_T}\mathbf{x}_0, (1-\alpha_T)\mathrm{I}) \\

& \qquad \qquad q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}\big(\sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\big) \quad (t > 1)

\end{align}
$$

ğŸ” ìœ„ì—ì„œ ì •ì˜í•œ inference processê°€ ëª¨ë“  $t$ì—ì„œ ì„±ë¦½í•¨ì„ ê·€ë‚©ë²•ì„ í†µí•´ ì¦ëª…í•´ì•¼ í•©ë‹ˆë‹¤.

1. $q\_\sigma(\mathbf{x}\_T\|\mathbf{x}\_0) = \mathcal{N}(\sqrt{\alpha\_T}\mathbf{x}\_0, (1-\alpha\_T)\mathrm{I})$ìœ¼ë¡œë¶€í„° $q\_\sigma(\mathbf{x}\_t\|\mathbf{x}\_0)$ì™€ $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_0)$ì„ ì •ì˜

   $$
   \begin{align}
       q_\sigma(\mathbf{x}_t|\mathbf{x}_0)\ =&\ \mathcal{N}(\sqrt{\alpha_t}\mathbf{x}_0, (1-\alpha_t)\mathrm{I}) \\
       q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)\ =&\ \mathcal{N}(\sqrt{\alpha_{t-1}}\mathbf{x}_0, (1-\alpha_{t-1})\mathrm{I})
   \end{align}
   $$

2. $t$ë¡œë¶€í„° $t-1$ë¥¼ ìœ ë„í•  ìˆ˜ ìˆìœ¼ë©´ ëª¨ë“  $t$ì—ì„œ ì„±ë¦½í•¨ì„ ì¦ëª…

   - Bishop 2.115ë²ˆ ê³µì‹ì„ ë¨¼ì € ì•Œì•„ë´ì•¼ í•©ë‹ˆë‹¤.

     - $p(\mathbf{x})$ê°€ Gaussianì´ê³  $p(\mathbf{y}\|\mathbf{x})$ ë˜í•œ Gaussian distributionì¼ ë•Œ $p(\mathbf{y})$ëŠ” Gaussian

   - Bishop 2.115ë²ˆ ê³µì‹ê³¼ ìœ„ì—ì„œ ì •ì˜í•œ ë‚´ìš©ì„ ì—°ê²° ì‹œì¼œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

     $$
     \begin{align}
         &p(\mathbf{x}) &\iff& \qquad q_{\sigma}(\mathbf{x}_t|\mathbf{x}_0)& \\
         &p(\mathbf{y}|\mathbf{x}) &\iff& \qquad q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \\
         &p(\mathbf{y}) &\iff& \qquad q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)& \\
     \end{align}
     $$

   - ìœ„ì—ì„œ $q\_{\sigma}(\mathbf{x}\_t\|\mathbf{x}\_0)$ì™€ $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_t, \mathbf{x}\_0)$ë¥¼ Gaussianìœ¼ë¡œ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— Bishop 2.115ë²ˆ ê³µì‹ì— ë”°ë¼ì„œ $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_0)$ëŠ” Gaussian ì´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ í‘œê¸°ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

     $$
     \begin{align}
         & q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0) = \mathcal{N}(\mu_{t-1}, \Sigma_{t-1}) \\ \\

           & \qquad \text{where} \\

           & \qquad \qquad q_\sigma(\mathbf{x}_t|\mathbf{x}_0)\ =\ \mathcal{N}(\sqrt{\alpha_t}\mathbf{x}_0, (1-\alpha_t)\mathrm{I}) \\

           & \qquad \qquad q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}\big(\sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\big) \quad (t > 1)
     \end{align}
     $$

   - $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_t, \mathbf{x}\_0)$ì— $\mathbf{x}\_t$ë¥¼ ëŒ€ì…í•´ì„œ $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_0)$ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•©ë‹ˆë‹¤.

     - ìœ„ì—ì„œ ì •ì˜í•œ $q\_\sigma(\mathbf{x}\_t\|\mathbf{x}\_0)\ =\ \mathcal{N}(\sqrt{\alpha\_t}\mathbf{x}\_0, (1-\alpha\_t)\mathrm{I})$ë¡œ ë¶€í„° reparameterization trickì„ í†µí•´ $\mathbf{x}\_t$ë¥¼ êµ¬í•©ë‹ˆë‹¤.

     $$
     \begin{align}
         \mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_0\ +\ \sqrt{1-\alpha_t}\epsilon_t
     \end{align}
     $$

     - $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_t, \mathbf{x}\_0)$ì— $\mathbf{x}\_t$ ëŒ€ì…í•˜ì—¬ í‰ê·  íŒŒíŠ¸ì™€ ë¶„ì‚° íŒŒíŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê³„ì‚°í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

     $$
     \begin{align}
         \mu_{t-1}
         &= \sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}} \\
         &= \sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\sqrt{\alpha_t}\mathbf{x}_0 - \sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}} \\
         &= \sqrt{\alpha_{t-1}}\mathbf{x}_0 \\

           \Sigma_{t-1}
         &= \sigma^2_t\mathrm{I}\ +\ (1-\alpha_{t-1} - \sigma^2_t) \cdot \frac{1-\alpha_t}{1-\alpha_t} \mathrm{I} \\
         &= (1-\alpha_{t-1})\mathrm{I}
     \end{align}
     $$

     - ë”°ë¼ì„œ $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_0)\ =\ \mathcal{N}(\sqrt{\alpha\_{t-1}}\mathbf{x}\_0, (1-\alpha\_{t-1})\mathrm{I})$ ì…ë‹ˆë‹¤.

   - $t$ë¡œë¶€í„° $t-1$ë¥¼ ìœ ë„í•˜ì˜€ìœ¼ë¯€ë¡œ ê·€ë‚©ë²•ì— ì˜í•´ì„œ ëª¨ë“  $t$ì—ì„œ ì„±ë¦½í•¨ì„ ì¦ëª…í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

3. ìœ„ì—ì„œ ì°¾ì€ $q\_{\sigma}(\mathbf{x}\_t\|\mathbf{x}\_0)$, $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_0)$, $q\_\sigma(\mathbf{x}\_{t-1}\|\mathbf{x}\_t, \mathbf{x}\_0)$ì„ Bayes' ruleì„ í†µí•´ $q\_\sigma(\mathbf{x}\_{t}\|\mathbf{x}\_{t-1}, \mathbf{x}\_0)$ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   $$
   q_\sigma(\mathbf{x}_{t}|\mathbf{x}_{t-1}, \mathbf{x}_0) = q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0) \times \frac{q_{\sigma}(\mathbf{x}_t|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)}
   $$

4. 3ë²ˆì—ì„œ ì°¾ì€ Bayes' ruleì„ í†µí•´ inference processë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   $$
   q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_0) := q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod^T_{t=2}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)
   $$

![](../../assets/img/Paper_Reading/DDIM/ddim_7.jpg){: .normal}

ğŸ” DDPMì˜ diffusion processì™€ ë‹¬ë¦¬ DDIMì˜ forward process(inference process)ëŠ” ë” ì´ìƒ Markovianì´ ì•„ë‹™ë‹ˆë‹¤.

- ë” ì´ìƒ forward processê°€ Markov processê°€ ì•„ë‹ˆë¯€ë¡œ diffusion processë¼ê³  ë¶€ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ [Section 3](#3-variational-inference-for-non-markovian-forward-process)ì—ì„œ ì •ë¦¬í•œ ë‚´ìš©ì— ì•½ê°„ì˜ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

  - Inference process : ~~Diffusion process~~ / Forward process
  - Reverse of inference process : ~~Denoising process~~ / Reverse process / <u>Generative process</u>

ğŸ” $\sigma$ì˜ í¬ê¸°ëŠ” forward processê°€ stochastic ì •ë„ë¥¼ í†µì œí•©ë‹ˆë‹¤.

- $\sigma \rightarrow 0$ : ì„ì˜ì˜ $t$ì— ëŒ€í•´ $\mathbf{x}\_{0}$ì™€ $\mathbf{x}\_{t}$ë¥¼ ë°œê²¬í•˜ëŠ” í•œ í•˜ë‚˜ì˜ $\mathbf{x}\_{t-1}$ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê·¹ë‹¨ì ì¸ ê²½ìš°ì— ë„ë‹¬í•©ë‹ˆë‹¤.

  > ğŸ’­ DDPMì—ì„œëŠ” ë‹¤ì–‘í•œ $\mathbf{x}\_{t}$ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆëŠ”ë° DDIMì—ì„œëŠ” Non-Markovianì„ í†µí•´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” $\mathbf{x}\_{t}$ì˜ ê²½ìš°ì˜ ìˆ˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ê²ƒì„ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ $\sigma$ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
  >
  > ğŸ’­ ê·¸ ì¤‘ì—ì„œ $\sigma$ê°€ 0ì¼ ë•Œ $\mathbf{x}\_{t}$ì˜ ê²½ìš°ì˜ ìˆ˜ë¥¼ í•˜ë‚˜ë¡œ ê³ ì •í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.
  >
  > ğŸ’­ ì´ì²˜ëŸ¼ $\sigma$ë¥¼ ì¡°ì ˆí•´ì„œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ê°€ëŠ” ê³¼ì •ì„ reverse process ëŒ€ì‹  generative processë¼ ë¶€ë¥´ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

### 3.2 Generative Process And Unified Vairational Inference Objective

![](../../assets/img/Paper_Reading/DDIM/ddim_8.jpg){: .normal}

ğŸ” í›ˆë ¨ ê°€ëŠ¥í•œ generative process $p\_{\theta}(\mathbf{x}\_{0:T})$ì—ì„œ ê° $p\_{\theta}^{(t)}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t})$ëŠ” $q\_{\sigma}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ì„ í™œìš©í•©ë‹ˆë‹¤.

ğŸ” $p\_{\theta}^{(t)}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t})$ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

1. $\mathbf{x}\_{0} \sim q(\mathbf{x}\_{0})$ì´ê³  $\epsilon\_{t} \sim \mathcal{N}(\mathrm{0}, \mathrm{I})$ ì¼ ë•Œ ì•ì—ì„œ ì •ì˜í•œ ì‹ì„ reparameterization trickì„ í™œìš©í•´ $\mathbf{x}\_t$ë¥¼ êµ¬í•©ë‹ˆë‹¤.

   $$
    \mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_0\ +\ \sqrt{1-\alpha_t}\epsilon_t \qquad \because q_{\sigma}(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_t}\mathbf{x}_0, (1 -\alpha_t)\mathrm{I})
   $$

2. ëª¨ë¸ $\epsilon\_{\theta}^{t}(\mathbf{x}\_{t})$ì€ $\mathbf{x}\_{t}$ë¡œë¶€í„° $\epsilon\_{t}$ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•©ë‹ˆë‹¤.

   $$
        L_\gamma(\epsilon_\theta) := \sum^T_{t=1}\gamma_t\mathbb{E}_{\mathbf{x}_0\sim q(\mathbf{x}_0), \epsilon_T \sim \mathcal{N}(\mathrm{0}, \mathrm{I})}\Big[ \lVert \epsilon^{(t)}_\theta (\mathbf{x}_{t})\ -\ \epsilon_t \rVert^2_2 \Big]
   $$

3. ëª¨ë¸ $\epsilon\_{\theta}^{t}(\mathbf{x}\_{t})$ì„ $\mathbf{x}\_t = \sqrt{\alpha\_t}\mathbf{x}\_0\ +\ \sqrt{1-\alpha\_t}\epsilon\_t$ì—ì„œ $\epsilon\_t$ ëŒ€ì‹  ëŒ€ì…í•˜ê³  í•´ë‹¹ ì‹ì„ $\mathbf{x}\_0$ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

   $$
       \begin{align}
       f_{\theta}^{(t)}(\mathbf{x}_{t})
       &=\ \hat{\mathbf{x}}_0 \\
       &=\ (\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})) / \sqrt{\alpha_{t}}
       \end{align}
   $$

4. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   $$
   \begin{align}
   q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \hat{\mathbf{x}}_{0})
   &= q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t)) \\
   &= \mathcal{N}\big(\sqrt{\alpha_{t-1}}f_{\theta}^{(t)}(\mathbf{x}_{t}) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}f_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\big)
   \end{align}
   $$

5. ì´ì œ generative processë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.

   - $p\_{\theta}(\mathbf{x}\_{T}) = \mathcal{N}(\mathrm{0}, \mathrm{I})$ë¡œ ê³ ì •
   - Generative processê°€ ëª¨ë“  ê³³ì—ì„œ ì‘ë™í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ $t=1$ì¼ ë•Œ Gaussian noiseë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
     - ìœ„ì—ì„œ inference processë¥¼ ì •ì˜í•  ë•Œ $t$ê°€ 1ë³´ë‹¤ í´ ë•Œ $q\_{\sigma}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ì„ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— $t=1$ì¼ ë•Œ ë”°ë¡œ ì •ì˜ê°€ í•„ìš”í–ˆìŒ
     - $p\_{\theta}^{(1)}(\mathbf{x}\_{0}\|\mathbf{x}\_{1}) = \mathcal{N}(f\_\theta^{(1)}(\mathbf{x}\_1),\ \sigma\_1^2\mathrm{I})$
   - ë§ˆì§€ë§‰ìœ¼ë¡œ generative processë¥¼ ì •ì˜í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

     $$
         p_{\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t})=
         \begin{cases}
         \mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1),\ \sigma_1^2\mathrm{I}), & \text{if } t = 1 \\
         q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t)), & \text{otherwise}
         \end{cases}
     $$

![](../../assets/img/Paper_Reading/DDIM/ddim_9.jpg){: .normal}

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ objective functionì„ ì •ì˜í–ˆìŠµë‹ˆë‹¤.

$$
\begin{align}
J_{\sigma} :=& \ \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}[\log q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_{0}) - \log p_{\theta}(\mathbf{x}_{0:T})]  & \qquad \\
=&\ \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\Bigg[\bigg(\log q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod^T_{t=2}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0) \bigg) - \bigg(\log p_\theta(\mathbf{x}_T)\prod^T_{t=1}p_\theta^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_t) \bigg)\Bigg]  & \qquad \\
=&
\ \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\Bigg[ \log q_\sigma(\mathbf{x}_T|\mathbf{x}_0) + \sum^T_{t=2}\log q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x
}_{0}) - \sum^T_{t=1} \log p_\theta^{(t)} (\mathbf{x}_{t-1}|\mathbf{x}_t) - \log p_\theta(\mathbf{x}_T) \Bigg]  & \qquad \\
=&
\ \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\Bigg[ \log q_\sigma(\mathbf{x}_T|\mathbf{x}_0) + \sum^T_{t=2}\log \frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)}{p^{(t)}_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} - \log p^{(1)}_\theta(\mathbf{x}_0|\mathbf{x}_1) - \log p_\theta(\mathbf{x}_T) \Bigg]  & \qquad \\
\equiv&
\ \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\Bigg[ \sum^T_{t=2}\log \frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)}{p^{(t)}_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} - \log p^{(1)}_\theta(\mathbf{x}_0|\mathbf{x}_1)\Bigg] & \qquad \because \text{í•™ìŠµì— í•„ìš”í•˜ì§€ ì•Šì€ ë¶€ë¶„ ì‚­ì œ}\\
=&
\ \sum^T_{t=2}\mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\Bigg[\log \frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)}{p^{(t)}_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}\Bigg] - \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\big[\log p^{(1)}_\theta(\mathbf{x}_0|\mathbf{x}_1)\big] \\
=&
\ \sum^T_{t=2}D_{KL}(q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\ ||\ p^{(t)}_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) - \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\big[\log p^{(1)}_\theta(\mathbf{x}_0|\mathbf{x}_1)\big]
\end{align}
$$

- ìœ„ì˜ objective functionì—ì„œ $t > 1$ì¼ ë•Œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

  $$
  \begin{align}
  D_{KL}(q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\ ||\ p^{(t)}_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))
  &= D_{KL}(q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\ ||\ q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t))) & \qquad \\
  &= \mathbb{E}_{\mathbf{x}_{0},\ \mathbf{x}_{t}\ \sim\ q_{\sigma}(\mathbf{x}_{0},\ \mathbf{x}_{t})} \Bigg[\frac{1}{2\sigma^2_t}\lVert \mathbf{x}_0 - f^{(t)}_\theta(\mathbf{x}_t)\rVert^2_2\Bigg] & \qquad \because \text{ë¶„ì‚°ì´ ê°™ì€ ë‘ Gaussian distribution} \\
  &= \mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0 + \sqrt{1-\alpha_t}\epsilon_t} \Bigg[\frac{1}{2\sigma^2_t}\bigg\lVert \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}\epsilon_t}{\sqrt{\alpha_t}} - \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}\epsilon^{(t)}_\theta(\mathbf{x}_t)}{\sqrt{\alpha_t}} \bigg\rVert^2_2  \Bigg] \\
  &= \mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0 + \sqrt{1-\alpha_t}\epsilon_t}\Bigg[\frac{1-\alpha_t}{2\sigma^2_t\alpha_t}\bigg\lVert  \epsilon_t\ -\ \epsilon^{(t)}_\theta(\mathbf{x}_t)\bigg\rVert^2_2  \Bigg]
  \end{align}
  $$

- ì´ì œëŠ” objective functionì—ì„œ $t=1$ì¼ ë•Œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

  $$
  \begin{align}
  \mathbb{E}_{\mathbf{x}_{0},\ \mathbf{x}_{1}\ \sim\ q_{\sigma}(\mathbf{x}_{0},\ \mathbf{x}_{1})}\big[-\log p^{(1)}_\theta(\mathbf{x}_0|\mathbf{x}_1)\big]
  &\equiv \mathbb{E}_{\mathbf{x}_{0},\ \mathbf{x}_{1}\ \sim\ q_{\sigma}(\mathbf{x}_{0},\ \mathbf{x}_{1})}\Bigg[\frac{1}{2\sigma^2_1}\bigg\lVert\mathbf{x}_0 - f^{(1)}_\theta(\mathbf{x}_1) \bigg\rVert^2_2\Bigg] \\
  &= \mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_1=\sqrt{\alpha_1}\mathbf{x}_0 + \sqrt{1-\alpha_1}\epsilon_1} \Bigg[\frac{1}{2\sigma^2_1}\bigg\lVert \frac{\mathbf{x}_1 - \sqrt{1-\alpha_1}\epsilon_1}{\sqrt{\alpha_1}} - \frac{\mathbf{x}_1 - \sqrt{1-\alpha_1}\epsilon^{(1)}_\theta(\mathbf{x}_1)}{\sqrt{\alpha_1}} \bigg\rVert^2_2  \Bigg] \\
  &= \mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_1=\sqrt{\alpha_1}\mathbf{x}_0 + \sqrt{1-\alpha_1}\epsilon_1} \Bigg[\frac{1-\alpha_1}{2\sigma^2_1\alpha_1}\bigg\lVert  \epsilon_1\ -\ \epsilon^{(1)}_\theta(\mathbf{x}_1)\bigg\rVert^2_2  \Bigg]
  \end{align}
  $$

- ë”°ë¼ì„œ ìµœì¢… objective functionì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  $$
  \begin{align}
  J_\sigma &\equiv \sum^T_{t=2}D_{KL}(q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\ ||\ p^{(t)}_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) - \mathbb{E}_{\mathbf{x}_{0:T}\sim q_{\sigma}(\mathbf{x}_{0:T})}\big[\log p^{(1)}_\theta(\mathbf{x}_0|\mathbf{x}_1)\big] \\
  &= \sum^T_{t=2}\mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0 + \sqrt{1-\alpha_t}\epsilon_t}\Bigg[\frac{1-\alpha_t}{2\sigma^2_t\alpha_t}\bigg\lVert  \epsilon_t\ -\ \epsilon^{(t)}_\theta(\mathbf{x}_t)\bigg\rVert^2_2  \Bigg] + \mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_1=\sqrt{\alpha_1}\mathbf{x}_0 + \sqrt{1-\alpha_1}\epsilon_1} \Bigg[\frac{1-\alpha_1}{2\sigma^2_1\alpha_1}\bigg\lVert  \epsilon_1\ -\ \epsilon^{(1)}_\theta(\mathbf{x}_1)\bigg\rVert^2_2  \Bigg] \\
  &= \sum^T_{t=1}\mathbb{E}_{\mathbf{x}_{0}\ \sim\ q_{\sigma}(\mathbf{x}_{0}),\ \epsilon\ \sim \mathcal{N}(\mathrm{0},\ \mathrm{I}),\ \mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0 + \sqrt{1-\alpha_t}\epsilon_t}\Bigg[\frac{1-\alpha_t}{2\sigma^2_t\alpha_t}\bigg\lVert  \epsilon_t\ -\ \epsilon^{(t)}_\theta(\mathbf{x}_t)\bigg\rVert^2_2  \Bigg] \\
  &= \sum^T_{t=1}\frac{1-\alpha_t}{2\sigma^2_t\alpha_t}\mathbb{E}\bigg[\big\lVert  \epsilon_t\ -\ \epsilon^{(t)}_\theta(\mathbf{x}_t)\big\rVert^2_2  \bigg] \\
  &= L_\gamma \qquad \text{when }\gamma_t = \frac{1-\alpha_t}{2\sigma^2_t\alpha_t}
  \end{align}
  $$

- ìµœì¢… objective functionì„ ìƒëµ ì—†ì´ ë‚˜íƒ€ë‚¸ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  $$J_\sigma(\epsilon_\theta) = L_\gamma(\epsilon_\theta) + C$$

## 4. Sampling From Generalized Generative Processes

![](../../assets/img/Paper_Reading/DDIM/ddim_10.jpg){: .normal}

ğŸ” Objective function $L\_1$ì„ ì‚¬ìš©í•´ì„œ Markov processì™€ Non-Markovian forward processë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

ğŸ” ê·¸ë ‡ê¸° ë•Œë¬¸ì— pretrained DDPMì„ $L\_1$ì— ëŒ€í•œ í•´ê²°ì±…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê³  $\sigma$ë¥¼ ë³€ê²½í•˜ì—¬ ìƒˆë¡œìš´ generative processë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4.1 Denoising Diffusion Implicit Models

![](../../assets/img/Paper_Reading/DDIM/ddim_11.jpg){: .normal}

ğŸ” $\epsilon\_{t} \sim \mathcal{N}(\mathrm{0}, \mathrm{I})$ê°€ $\mathbf{x}\_t$ì™€ ë…ë¦½ì ì´ê³  $\alpha\_{0} := 1$ì´ë¼ í•  ë•Œ $\mathbf{x}\_t$ë¡œë¶€í„° $\mathbf{x}\_{t-1}$ì„ ìƒì„±í•˜ëŠ” ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\begin{align}
q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \hat{\mathbf{x}}_{0})
&= q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, f_\theta^{(t)}(\mathbf{x}_t)) & \qquad \\

&= \mathcal{N}\big(\sqrt{\alpha_{t-1}}f_{\theta}^{(t)}(\mathbf{x}_{t}) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}f_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{1-\alpha_t}}, \sigma^2_t\mathrm{I}\big) & \qquad \\

&= \sqrt{\alpha_{t-1}}f_{\theta}^{(t)}(\mathbf{x}_{t}) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}f_{\theta}^{(t)}(\mathbf{x}_{t})}{\sqrt{1-\alpha_t}} + \sigma_t \epsilon_t & \qquad \\

&= \sqrt{\alpha_{t-1}}\Bigg(\frac{\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}} \Bigg) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \frac{\mathbf{x}_t - \sqrt{\alpha_t}\big(\frac{\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}} \big)}{\sqrt{1-\alpha_t}} + \sigma_t \epsilon_t & \qquad \because f_{\theta}^{(t)}(\mathbf{x}_{t}) = \frac{\mathbf{x}_t - \sqrt{1-\alpha_{t}}\cdot \epsilon_{\theta}^{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}}\\

&= \sqrt{\alpha_{t-1}}\underset{\text{"predicted }\mathbf{x}_{0}\text{"}}{\underbrace{\bigg(\frac{\mathbf{x}_{t} - \sqrt{1-\alpha_{t}}\epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}}\bigg)}}\ +\ \underset{\text{"direction pointing to }\mathbf{x}_{t}\text{"}}{\underbrace{\sqrt{1 - \alpha_{t-1} - \sigma^{2}_{t}}\ \cdot\ \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}}\ +\ \underset{\text{random noise}}{\underbrace{\sigma_{t}\epsilon_{t}}} & \qquad \\


\end{align}
$$

ğŸ” ìœ„ì—ì„œ ìƒê°í–ˆë˜ ê²ƒì²˜ëŸ¼ $\sigma$ë¥¼ ë‹¤ë¥´ê²Œ ì„ íƒí•˜ë©´ ë‹¤ë¥¸ generative processê°€ ë°œìƒí•˜ë¯€ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµì‹œì¼œì•¼ í•©ë‹ˆë‹¤.

ğŸ” $\sigma\_t$ë¥¼ $\sqrt{(1-\alpha\_{t-1})/(1-\alpha\_{t})}\sqrt{1-\alpha_{t}/\alpha\_{t-1}}$ë¡œ ì„¤ì •í–ˆì„ ë•Œ forward processì™€ generative processëŠ” DDPMì´ ë©ë‹ˆë‹¤.

> ğŸ’¡ $\sigma\_{t} = \sqrt{(1-\alpha\_{t-1})/(1-\alpha\_{t})}\sqrt{1-\alpha\_{t}/\alpha\_{t-1}}$ì€ DDPMì—ì„œ $\sigma\_{t}$ ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” $\tilde{\beta}\_t$ë¥¼ DDIMì—ì„œ ì‚¬ìš©í•˜ëŠ” notationìœ¼ë¡œ ë³€ê²½í•œ ì‹

ğŸ” $\sigma\_t = 0$ì¼ ë•Œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

- $\mathbf{x}\_{t-1}$ê³¼ $\mathbf{x}\_{0}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ forward processëŠ” deterministicì´ ë©ë‹ˆë‹¤.

- Random noise($\sigma\_t\epsilon\_t$)ì—ì„œ $\epsilon\_t$ì˜ ê³„ìˆ˜ê°€ 0ì´ ë©ë‹ˆë‹¤.

- Resulting modelì€ implicit probabilistic modelì´ ë˜ë©° sampleì€ ê³ ì •ëœ ì ˆì°¨ë¡œ latent variablesì—ì„œ ìƒì„±ë©ë‹ˆë‹¤.

ğŸ’­ $\sigma\_t = 0$ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ í•˜ë‚˜ì˜ sampleì„ ìƒì„±í•  ë•Œ DDPMê³¼ ë‹¬ë¦¬ ê³ ì •ëœ latentë¥¼ ê°€ì§„ë‹¤ëŠ” ì˜ë¯¸ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

### 4.2 Accelerated Generation Processes

![](../../assets/img/Paper_Reading/DDIM/ddim_12.jpg){: .normal}

ğŸ” Denoising objective $L\_1$ì€ $q\_\sigma(\mathbf{x}\_t\|\mathbf{x}\_0)$ê°€ ê³ ì •ë˜ì–´ ìˆëŠ” í•œ íŠ¹ì • forward procedureì— ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

> ğŸ’­ íŠ¹ì • forward procedureì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ë‹¤ëŠ” ê²ƒì„ ë‘ ê°€ì§€ ì¸¡ë©´ìœ¼ë¡œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
>
> 1.  $q\_{\sigma}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ì„ ì˜ë¯¸ì ìœ¼ë¡œ ìƒê°í•´ë³´ê¸°
>     > $$q_{\sigma}(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \hat{\mathbf{x}}_{0}) = \sqrt{\alpha_{t-1}}\underset{\text{"predicted }\mathbf{x}_{0}\text{"}}{\underbrace{f_{\theta}^{(t)}(\mathbf{x}_{t})}}\ +\ \underset{\text{"direction pointing to }\mathbf{x}_{t}\text{"}}{\underbrace{\sqrt{1 - \alpha_{t-1} - \sigma^{2}_{t}}\ \cdot\ \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}}\ +\ \underset{\text{random noise}}{\underbrace{\sigma_{t}\epsilon_{t}}}$$
>     >
>     > - ìœ„ì˜ ì‹ì€ Prdicted $\mathbf{x}\_0$, Direction pointing to $\mathbf{x}\_t$, Random noiseë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.
>     >
>     > - $\mathbf{x}\_0$ë¥¼ ì˜ˆì¸¡í•˜ê³  ì˜ˆì¸¡ëœ $\mathbf{x}\_0$ì„ $\mathbf{x}\_t$ì˜ ë°©í–¥ìœ¼ë¡œ ì´ë™ì‹œí‚¨ í›„ random noiseë¥¼ ì¶”ê°€í•´ì„œ $\mathbf{x}\_{t-1}$ì„ êµ¬í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>     >
>     > - ê·¸ë ‡ë‹¤ë©´ $\mathbf{x}\_0$ë¥¼ ì˜ˆì¸¡í•˜ê³  $\mathbf{x}\_t$ë¥¼ êµ¬í•˜ê¸°ë§Œ í•œë‹¤ë©´ random noiseë¥¼ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ $\mathbf{x}\_{t-1}$ ë¿ë§Œ ì•„ë‹ˆë¼ $\mathbf{x}\_{t-2}$ë„ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ê¸° ë•Œë¬¸ì— denoising objectiveëŠ” íŠ¹ì • forward procedureì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
> 2.  $q\_{\sigma}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ê°€ ì–´ë–»ê²Œ ì •ì˜ ë˜ì—ˆëŠ”ì§€ ìˆ˜ì‹ì ìœ¼ë¡œ ì‚´í´ë³´ê¸°
>     > - ì´ì „ì— $q\_{\sigma}(\mathbf{x}\_{t}\|\mathbf{x}\_{0})$ê°€ ëª¨ë“  $t$ì— ëŒ€í•´ì„œ ë§Œì¡±í•¨ì„ ë³´ì¼ ë•Œ $q\_{\sigma}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ê°€ ì•„ë‹Œ $q\_{\sigma}(\mathbf{x}\_{t-2}\|\mathbf{x}\_{t}, \mathbf{x}\_{0}) = \mathcal{N}\big(\sqrt{\alpha\_{t-2}}\mathbf{x}\_0 + \sqrt{1-\alpha\_{t-2} - \sigma^2\_t}\cdot \frac{\mathbf{x}\_t - \sqrt{\alpha\_t}\mathbf{x}\_0}{\sqrt{1-\alpha\_t}}, \sigma^2\_t\mathrm{I}\big)$ë¼ê³  í–ˆì„ ë•Œ $q\_{\sigma}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{0})$ì´ ì•„ë‹Œ $q\_{\sigma}(\mathbf{x}\_{t-2}\|\mathbf{x}\_{0})$ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>     >
>     > - í•´ë‹¹ ì‹ì„ subsequenceì— ëŒ€í•´ Bayes'ruleë¡œ ì²˜ìŒ inference process(forward process)ë¥¼ ì •ì˜í•œ í›„ generative processë¥¼ ì •ì˜í•˜ê²Œ ë˜ë©´ $T$ë³´ë‹¤ ì§¦ì€ subsequence ê¸¸ì´ ë§Œí¼ë§Œìœ¼ë¡œë„ sampleì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>     >
>     > - ê·¸ë ‡ê¸° ë•Œë¬¸ì— $q\_\sigma(\mathbf{x}\_t\|\mathbf{x}\_0)$ë§Œ ë³€ê²½í•˜ì§€ ì•Šìœ¼ë©´ (ê³ ì •ë˜ì–´ ìˆìœ¼ë©´) denoising objectiveëŠ” íŠ¹ì • forward procedureì— ì˜ì¡´í•˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.

### 4.3 Relevance to Neural ODEs

![](../../assets/img/Paper_Reading/DDIM/ddim_13.jpg){: .normal}

ğŸ” DDIMì€ diffusionì„ discreteí•˜ê²Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ ë°©ì •ì‹ì´ë¼ ë³´ê¸´ í˜ë“¤ì§€ë§Œ Euler's methodë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” sampling í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

- Sampling

  $$\mathbf{x}_{t-1} = \sqrt{\alpha_{t-1}}{\bigg(\frac{\mathbf{x}_{t} - \sqrt{1-\alpha_{t}}\epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}}\bigg)}\ +\ \sqrt{1 - \alpha_{t-1} - \sigma^{2}_{t}}\ \cdot\ \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})\ +\ \sigma_{t}\epsilon_{t}$$

- DDIMì€ sampling ì‹ì—ì„œ $\sigma\_t = 0$ìœ¼ë¡œ ë‘ì–´ì•¼ í•©ë‹ˆë‹¤.

  $$
  \begin{align}
  \frac{\mathbf{x}_{t-1}}{\sqrt{\alpha_{t-1}}}
  &= \frac{\mathbf{x}_{t} - \sqrt{1-\alpha_{t}}\epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}} + \frac{\sqrt{1 - \alpha_{t-1}}\ \cdot\ \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})}{\sqrt{\alpha_{t-1}}} \\
  &= \frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}} + \Bigg(\frac{\sqrt{1 - \alpha_{t-1}}}{\sqrt{\alpha_{t-1}}} - \frac{\sqrt{1-\alpha_{t}}}{\sqrt{\alpha_{t}}}\Bigg)\cdot \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})
  \end{align}
  $$

- Discreteí•˜ê¸° ë•Œë¬¸ì— ì‹œê°„ ê°„ê²©ì„ ì „ë¶€ $\Delta$ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.

  $$\frac{\mathbf{x}_{t - \Delta t}}{\sqrt{\alpha_{t-\Delta t}}} = \frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}} + \Bigg(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}} - \sqrt{\frac{1-\alpha_{t}}{\alpha_{t}}} \Bigg)\cdot \epsilon^{(t)}_{\theta}(\mathbf{x}_{t})$$

- Euler's methodë¥¼ ì ìš©í•˜ê¸° ìœ„í•œ ODEë¡œ ë§Œë“¤ê¸° ìœ„í•´ reparameterizationì„ í•©ë‹ˆë‹¤.

  $$\frac{\sqrt{1-\alpha}}{\sqrt{\alpha}} = \sigma\ , \qquad \frac{\mathbf{x}}{\sqrt{\alpha}} = \bar{\mathbf{x}}$$

- Discreteí•œ sampling ì‹ì— reaparameterizationì„ ì ìš©í•©ë‹ˆë‹¤.

  $$
  \begin{align}
  \frac{\mathbf{x}_{t - \Delta t}}{\sqrt{\alpha_{t-\Delta t}}}\qquad
  =\qquad &\frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}} + \Bigg(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}} - \sqrt{\frac{1-\alpha_{t}}{\alpha_{t}}} \Bigg)\cdot \epsilon^{(t)}_{\theta}(\mathbf{x}_{t}) \\

  \frac{\mathbf{x}_{t - \Delta t}}{\sqrt{\alpha_{t-\Delta t}}} - \frac{\mathbf{x}_{t}}{\sqrt{\alpha_{t}}}\qquad
  =\qquad &\Bigg(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}} - \sqrt{\frac{1-\alpha_{t}}{\alpha_{t}}} \Bigg)\cdot \epsilon^{(t)}_{\theta}(\mathbf{x}_{t}) \\

  \mathrm{d}\bar{\mathbf{x}}(t)\qquad
  =\qquad &\mathrm{d}\sigma(t)\cdot \epsilon^{(t)}_{\theta}(\mathbf{x}_{t}) \\

  \mathrm{d}\bar{\mathbf{x}}(t)\qquad
  =\qquad & \epsilon^{(t)}_{\theta}(\mathbf{x}_{t}) \mathrm{d}\sigma(t)\\

  \mathrm{d}\bar{\mathbf{x}}(t)\qquad
  =\qquad & \epsilon^{(t)}_{\theta}\Bigg(\frac{\bar{\mathbf{x}}}{\sqrt{\sigma^2 + 1}}\Bigg) \mathrm{d}\sigma(t)\\
  \end{align}
  $$

ğŸ” ìˆ˜ì •í•œ sampling ì‹ì€ ODEì´ë¯€ë¡œ Euler's methodë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ ì´ˆê¸°ê°’ì€ ë§¤ìš° í° $\sigma(T)$ì— ëŒ€í•˜ì—¬ $\mathbf{x}(T) \sim \mathcal{N}(0, \sigma(T))$ì…ë‹ˆë‹¤.

ğŸ” ì´ëŠ” discretization stepì´ ì¶©ë¶„í•˜ë©´ $\mathbf{x}\_{0}$ë¥¼ $\mathbf{x}\_{T}$ë¡œ ì¸ì½”ë”© í•˜ê³  ìœ„ì˜ ìˆ˜ì •ëœ sampling ì‹ì˜ ì—­ì„ í†µí•´ ë””ì½”ë”© í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸ í•©ë‹ˆë‹¤.

ğŸ” ë˜í•œ $\mathbf{x}\_{0}$ì˜ ì¸ì½”ë”© ê°’ $\mathbf{x}\_{T}$ì„ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì˜ latent representationsë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ë‹¤ë¥¸ downstream applicationsì— ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ğŸ’­ GANsì²˜ëŸ¼ latent interpolationì´ ê°€ëŠ¥í•˜ì§€ ì•Šì„ê¹Œ í•©ë‹ˆë‹¤.

## 5. Experiments

![](../../assets/img/Paper_Reading/DDIM/ddim_14.jpg){: .normal}

ğŸ” í•´ë‹¹ sectionì—ì„œëŠ” ì•„ë˜ì˜ ë‚´ìš©ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

1. DDPMë³´ë‹¤ ë¹ ë¥¸ ì†ë„ì˜ ì´ë¯¸ì§€ ìƒì„± ([Section 5.1](#51-sample-quality-and-efficiency))
2. DDPMê³¼ ë‹¬ë¦¬ initial latent variables $\mathbf{x}\_{T}$ê°€ ê³ ì •ë˜ë©´ DDIMì€ high-level image featureë¥¼ ìœ ì§€ ([Section 5.2](#52-sample-consistency-in-ddims))
3. DDIMì€ latent spaceì—ì„œ ì§ì ‘ interpolation ([Section 5.3](#53-interpolation-in-detecministic-generative-processes))
4. Latent codeë¡œë¶€í„° reconstruction ([Section 5.4](#54-reconstruction-from-latent-space))
5. Deterministic DDIMê³¼ stochastic DDPM ì‚¬ì´ë¥¼ ë³´ê°„í•˜ëŠ” $\tau$ì™€ $\sigma$ë¥¼ ì œì–´ ([Section 5.1](#51-sample-quality-and-efficiency))

ğŸ” ì‹¤í—˜ë“¤ì˜ ë¹„êµë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ $\sigma$ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê³ ë ¤í•©ë‹ˆë‹¤. ì´ë•Œ, $\eta$ëŠ” ì§ì ‘ ì œì–´ê°€ ê°€ëŠ¥í•œ hyperparameterì´ë©° $eta$ë¥¼ ì¡°ì ˆí•´ì„œ DDPMê³¼ DDIMì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$\sigma_{\tau_i}(\eta) = \eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$$

ğŸ” $\sigma = 1$ë³´ë‹¤ í° standard deviationì„ ê°–ëŠ” random noiseë¥¼ ì‚¬ìš©í•œ DDPMì„ ê³ ë ¤í•©ë‹ˆë‹¤.

$$\hat{\sigma}:\ \hat{\sigma}_{\tau_i} = \sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$$

### 5.1 Sample Quality And Efficiency

![](../../assets/img/Paper_Reading/DDIM/ddim_15.jpg){: .normal}

ğŸ” Sample qualityëŠ” $\text{dim}(\tau)$ë¥¼ ì¦ê°€ì‹œí‚´ì— ë”°ë¼ ë” ì¢‹ì•„ì§‘ë‹ˆë‹¤.

ğŸ” Sample Qualityì™€ Computation costëŠ” ë°˜ë¹„ë¡€

ğŸ” DDPMì˜ ê²½ìš° ê¸‰ê²©íˆ sample qualityê°€ ì•ˆì¢‹ì•„ì§€ëŠ” ë°˜ë©´ DDIMì€ ì¼ê´€ë˜ê²Œ ì¢‹ì€ sample qualityë¥¼ ìœ ì§‘í•©ë‹ˆë‹¤.

![](../../assets/img/Paper_Reading/DDIM/ddim_16.jpg){: .normal}

ğŸ” ë™ì¼í•œ $\text{dim}(\tau)$ì—ì„œ $\sigma$ê°€ ë‹¬ë¼ì§ì— ë”°ë¼ ìƒì„±ëœ sampleì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](../../assets/img/Paper_Reading/DDIM/ddim_17.jpg){: .normal}

ğŸ” $\text{dim}(\tau)$ì— ë”°ë¼ sampleì„ ìƒì„±í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### 5.2 Sample Consistency In DDIMs

![](../../assets/img/Paper_Reading/DDIM/ddim_18.jpg){: .normal}

ğŸ” ë™ì¼í•œ $\mathbf{x}\_T$ìœ¼ë¡œ ì‹œì‘í•˜ë©´ì„œ ì„œë¡œ ë‹¤ë¥¸ generative trajectories($\tau$)ì—ì„œ ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” ë§ì€ ê²½ìš° 20 stepsë¡œ ìƒì„±ëœ sampleì€ ì´ë¯¸ 1000 stepsë¡œ ìƒì„±ëœ sampleê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë©° ì„¸ë¶€ ì‚¬í•­ì— ìˆì–´ì„œ ì•½ê°„ì˜ ì°¨ì´ë§Œ ìˆìŠµë‹ˆë‹¤.

ğŸ” ë”°ë¼ì„œ $\mathbf{x}\_T$ë§Œì´ $\mathbf{x}_0$ì˜ ìœ ì¼í•œ latent encodingì…ë‹ˆë‹¤.

ğŸ” $\text{dim}(\tau)$ì´ í´ìˆ˜ë¡ ì¦‰, timestepì´ í´ìˆ˜ë¡ ë” ì¢‹ì€ sample qualityë¥¼ ë³´ì´ì§€ë§Œ timestepì˜ ê¸¸ì´ëŠ” high-level featureì™€ëŠ” ìƒê´€ì´ ì—†ì–´ ë³´ì…ë‹ˆë‹¤.

### 5.3 Interpolation In Detecministic Generative Processes

![](../../assets/img/Paper_Reading/DDIM/ddim_19.jpg){: .normal}

ğŸ” $\mathbf{x}\_T$ì— ì˜í•´ì„œ ì¸ì½”ë”© ë˜ê¸° ë•Œë¬¸ì— GANsê³¼ ê°™ì´ semantic interpolationì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.

### 5.4 Reconstruction From Latent Space

![](../../assets/img/Paper_Reading/DDIM/ddim_20.jpg){: .normal}

ğŸ” Reconstruction errorë¥¼ ê³„ì‚° í–ˆì„ ë•Œ DDIMì€ ë” í° $S$ì— ëŒ€í•´ ë” ë‚®ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.
