---
layout: post
title: "[Vision Transformer] ViT : An image is worth 16X16 words: Transformers for image recognition at scale"
author: kjy
date: 2023-09-30 17:25:00 +09:00
categories: [Paper Review, CV, Vision Transformer]
tags: [Paper Review, CV, Vision Transformer]
comments: true
toc: true
math: true
---

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [An image is worth 16X16 words: Transformers for image recognition at scale(ViT)](https://arxiv.org/pdf/2010.11929.pdf) ë…¼ë¬¸ ë¦¬ë·°ë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

## â… . Abstract

í•´ë‹¹ ë…¼ë¬¸ì€ NLPë¶„ì•¼ì˜ transformerë¥¼ computer vision ë¶„ì•¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê³ ì•ˆëœ ëª¨ë¸ ViTì— ëŒ€í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. ViTëŠ” ì›ë³¸ ì´ë¯¸ì§€ì— ë³€í˜•ì„ ê°€í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ input dataë¡œ ì‚¬ìš©í•˜ë˜ CNNì¸ ì•„ë‹Œ full self-attention êµ¬ì¡°ë¥¼ computer visionì— ì‚¬ìš©ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.

í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” classification taskì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¨ì—ˆìœ¼ë©° ë‹¤ë¥¸ computer vision ë¶„ì•¼ì¸ object detection, semantic segmentationê³¼ ê°™ì€ ë¶„ì•¼ì— ëŒ€í•´ì„œëŠ” ì¶”í›„ ì—°êµ¬ë¡œ ë‚¨ê²¼ìŠµë‹ˆë‹¤.

ë˜í•œ ë§ì€ ì–‘ì˜ dataì— ëŒ€í•´ pre-train í•˜ê³  ì ì€ dataë¡œ fine-tune í–ˆì„ ë•Œ CNN SOTAë³´ë‹¤ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ë©´ì„œ computational cost ì¸¡ë©´ì—ì„œë„ íš¨ìœ¨ì ì„ì„ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ë°í˜”ìŠµë‹ˆë‹¤.

í•´ë‹¹ ë¸”ë¡œê·¸ì—ì„œëŠ” ViTì˜ êµ¬ì¡°ë¥¼ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬ë¥¼ í•´ë³´ê³  ìœ„ì—ì„œ ë§í•œ ViTì˜ íŠ¹ì§•ë“¤ì— ëŒ€í•œ ì‹¤í—˜ë“¤ì„ ìœ„ì£¼ë¡œ ì •ë¦¬ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ ì™¸ì˜ ë‹¤ë¥¸ ë‚´ìš©ë“¤ì€ ë…¼ë¬¸ì„ ì§ì ‘ ì°¸ê³ í•´ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

## â…¡. ViT êµ¬ì¡°

ë¨¼ì € ViTë¥¼ ê°„ë‹¨í•˜ê²Œ ì˜ ì„¤ëª…í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

![](../../assets/img/ViT/vit_1.gif){: class="align-center"}
_ì¶œì²˜: [https://github.com/lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch)_

ViTëŠ” NLP ë¶„ì•¼ì˜ transformer êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ê³ ì í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ transformerì˜ êµ¬ì¡°ë¥¼ ì•„ì‹œë©´ ViTì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ” ë¶€ë¶„ì— ìˆì–´ì„œëŠ” í° ì–´ë ¤ì›€ì´ ì—†ì„ ê²ƒì´ë¼ ì˜ˆìƒí•©ë‹ˆë‹¤. ë‹¤ë§Œ NLP ë¶„ì•¼ì˜ transformerì™€ ë‹¤ë¥¸ ì ì´ë¼ê³  í•œë‹¤ë©´ í¬ê²Œ encdoer-decoder êµ¬ì¡°ê°€ ì•„ë‹Œ encoderë§Œ ì‚¬ìš©í•œë‹¤ëŠ” ì ê³¼ CLASS tokenì´ ì¶”ê°€ëœë‹¤ëŠ” ì ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. Image patchì™€ ê°™ì´ ì´ë¯¸ì§€ì— ë§ê²Œ ë³€ê²½ëœ ë¶€ë¶„ì´ ìˆê¸°ëŠ” í•˜ì§€ë§Œ ìµœëŒ€í•œ NLP ë¶„ì•¼ì˜ transformerì˜ êµ¬ì¡°ë¥¼ ë”°ë¼ê°€ê³ ì í•œ ê²ƒì´ ëˆˆì— ë„ëŠ” ëª¨ë¸ êµ¬ì¡°ì…ë‹ˆë‹¤.

### ViT input

1. Patch Embedding

   - ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.

     > - Patchì˜ í¬ê¸° : $P$
     > - Patchì˜ ê°¯ìˆ˜ : $N = HW/P^2$
     > - Channel ìˆ˜ : $C$
     >
     > ![](../../assets/img/ViT/vit_2.png){: class="align-center"}

   - Latent vector size $D$ë¡œ mapping ì‹œì¼œì¤ë‹ˆë‹¤.

     > ![](../../assets/img/ViT/vit_3.png){: class="align-center"}

2. CLASS Token ì¶”ê°€

   ì´ë¯¸ì§€ì˜ classë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ CLASS Tokenì„ $D$ì°¨ì›ì˜ vectorë¡œ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.

   > ![](../../assets/img/ViT/vit_4.png){: class="align-center"}

3. Position embedding

   Position embeddingì„ ëª¨ë¸ì— ì£¼ì…í•˜ëŠ” ìœ„ì¹˜, ë°©ë²•ì„ ë‹¤ë¥´ê²Œí•˜ì—¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ê³ ì í–ˆìœ¼ë‚˜ ì €ìë“¤ì˜ ì‹¤í—˜ ê²°ê³¼ position embeddingì´ ëª¨ë¸ì— ì£¼ì…í•˜ê¸°ë§Œ í•œë‹¤ë©´ ëª¨ë¸ì— ì£¼ì…í•˜ëŠ” ìœ„ì¹˜ëŠ” í° ìƒê´€ì´ ì—†ì—ˆê³  2D-positional embeddingì™€ relative positional embeddingì„ ì‹¤í—˜í–ˆìœ¼ë‚˜ í° ì„±ëŠ¥ í–¥ìƒì„ ë°œê²¬í•˜ì§€ ëª»í•´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” 1D-positional embeddingì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

   ![](../../assets/img/ViT/vit_5.png){: class="align-center"}

4. í•œ ëˆˆì— ë³´ê¸°

   Patch Embedding, CLASS Token, position embeddingì„ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

   $$
   \mathbf{z}0 = [\mathbf{x}{\text{class}}; \mathbf{x}^1_p\mathbf{E}; \mathbf{x}^2_p\mathbf{E};\cdots ; \mathbf{x}^N_p\mathbf{E}] + \mathbf{E}{pos},\quad \mathbf{E} \in \mathbb{R}^{(P^2\cdot C)\times D},\ \mathbf{E}{pos} \in \mathbb{R}^{(N+1)\times D}
   $$

   Patch Embedding, CLASS Token, position embeddingì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

   ![](../../assets/img/ViT/vit_6.png){: class="align-center"}

### Encoder & Classification

Inputì˜ í˜•íƒœë¥¼ NLPë¶„ì•¼ì˜ transformer inputì˜ í˜•íƒœì™€ ë˜‘ê°™ì´ ë§Œë“¤ì—ˆê¸° ë•Œë¬¸ì— ì´í›„ ê³„ì‚° ê³¼ì •ì€ í¬ê²Œ ë‹¤ë¥¸ ê²ƒì´ ì—†ìœ¼ë¯€ë¡œ ìˆ˜ì‹ìœ¼ë¡œë§Œ ì•Œì•„ë³´ê³  ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ìˆ˜ì‹ì€ inputë¶€í„° classificationê¹Œì§€ì˜ ê³¼ì •ì„ ëª¨ë‘ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤.

$$
\begin{aligned}
&\mathbf{z}0 = [\mathbf{x}{\text{class}}; \mathbf{x}^1_p\mathbf{E}; \mathbf{x}^2_p\mathbf{E};\cdots ; \mathbf{x}^N_p\mathbf{E}] + \mathbf{E}{pos},\quad
&&\mathbf{E} \in \mathbb{R}^{(P^2\cdot C)\times D},\ \mathbf{E}{pos} \in \mathbb{R}^{(N+1)\times D}\quad&(1)\\

&\mathbf{z}^\prime_\ell = \text{MSA}(\text{LN}(\mathbf{z}_{\ell-1}))+\mathbf{z}_{\ell-1},
&&\ell = 1 \dots L\quad&(2)\\

&\mathbf{z}_\ell = \text{MSA}(\text{LN}(\mathbf{z}^\prime_{\ell}))+\mathbf{z}^\prime_{\ell},
&&\ell = 1 \dots L\quad&(3)\\

&\mathbf{y} = \text{LN}(\mathbf{z}^0_L)
&&
&(4)\\
\end{aligned}
$$

## â…¢. ViTì™€ CNN

í•´ë‹¹ ë…¼ë¬¸ì—ì„œ CNNê³¼ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œ ëª¨ë¸ì€ í¬ê²Œ 3ê°€ì§€ ì…ë‹ˆë‹¤.

1. ViT

   - ViT-B/P : Patchì˜ í¬ê¸°ê°€ $P$ì¸ ViT Base ëª¨ë¸
   - ViT-L/P : Patchì˜ í¬ê¸°ê°€ $P$ì¸ ViT Large ëª¨ë¸
   - ViT-H/P : Patchì˜ í¬ê¸°ê°€ $P$ì¸ ViT Huge ëª¨ë¸

   ğŸ” Transformerì˜ sequence lengthëŠ” $P^2$ì— ë°˜ë¹„ë¡€í•˜ë¯€ë¡œ $P$ê°€ ì‘ì„ìˆ˜ë¡ ì¦‰, patchê°€ pixelì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤.

   ![](../../assets/img/ViT/vit_7.png){: class="align-center"}

2. ResNet(BiT)

   - BiT : [Big Transfer](https://arxiv.org/abs/1912.11370)ì—ì„œ ì‚¬ìš©í•œ ResNetì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
   - ë‹¨, batch normalizationì„ group normalizationìœ¼ë¡œ ë³€ê²½
   - Standardized convolutions ì‚¬ìš©

3. Hybrid

   - Patch sizeê°€ 1ì¸ ViTì— intermediate feature map ê³µê¸‰
   - Intermediate feature map
     - ResNet50ì˜ 4ë‹¨ê³„ ì¶œë ¥ê°’
     - ResNet50ì˜ 4ë‹¨ê³„ë¥¼ ì œê±°í•˜ê³  3ë‹¨ê³„ì˜ ë™ì¼í•œ ìˆ˜ì˜ ì¸µì„ ë°°ì¹˜(ì „ì²´ ì¸µ ìˆ˜ ìœ ì§€)í•œ í™•ì¥ëœ 3ë‹¨ê³„ì˜ ì¶œë ¥

ìœ„ì˜ 3ê°€ì§€ ëª¨ë¸ì„ í†µí•´ ì €ìë“¤ì€ CNNë³´ë‹¤ ViTê°€ ê³„ì‚°ì  ì¸¡ë©´ ê·¸ë¦¬ê³  ì„±ëŠ¥ë©´ì—ì„œ ì¢‹ì€ ê²ƒì„ ë°íˆê³ ì í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì ì€ dataë¡œ í•™ìŠµí•˜ì—¬ í‰ê°€ë¥¼ í•  ë•Œ CNNì´ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ëŠ”ë° ì´ê²ƒì€ CNNì˜ inductive bias(localization, translation equivariance)ê°€ ViTì—ëŠ” ì ê¸° ë•Œë¬¸ì´ë¼ê³  ì €ìë“¤ì€ ë§í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ dataë¡œ í•™ìŠµí•˜ë©´ ì§ì ‘ ì´ë¯¸ì§€ë“¤ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ ë˜ë¯€ë¡œ inductive biasì™€ ìƒê´€ì—†ì´ CNNë³´ë‹¤ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

ë˜í•œ ë§ì€ dataë¡œ í•™ìŠµì„ í•œ í›„ fine-tuningì„ í•˜ê²Œ ë˜ë©´ CNNë³´ë‹¤ ê³„ì‚° ë¹„ìš©ì´ ì ê³  CNNë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

### SOTA ë¹„êµ

JFT datasetsìœ¼ë¡œ pre-trainì„ í•˜ê³  fine-tuning í–ˆì„ ë•Œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

![](../../assets/img/ViT/vit_8.png){: class="align-center"}

### Pre-training í•  ë•Œì˜ dataset sizeì˜ ì¤‘ìš”ì„±

1. `ImageNEt`, `ImageNet-21k`, `JFT-300M`ìœ¼ë¡œ pre-trainingì„ í•˜ê³  `CIFAR-10/100`, `ImageNet`, `ImageNet ReaL`, `Oxford`ë¡œ fine-tuning í–ˆì„ ë•Œì˜ ê²°ê³¼ë¥¼ ë¹„êµí•œ ì‹¤í—˜ì…ë‹ˆë‹¤. ê·¸ ì¤‘ `ImageNet`ìœ¼ë¡œ fine-tuning í–ˆì„ ë•Œì˜ ê²°ê³¼ë¥¼ ë„í‘œë¡œ ì‚´í´ë³´ë©´ pre-train datasetì´ ì»¤ì§ˆìˆ˜ë¡ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   ![](../../assets/img/ViT/vit_9.png){: class="align-center"}

2. `JFT-300M`ì„ random subsetìœ¼ë¡œ ë‚˜ëˆ„ì–´ pre-training í•  ë•Œ `ImageNet`ì— ëŒ€í•œ fine-tuning ì„±ëŠ¥ì„ 5-shotìœ¼ë¡œ ë¹„êµí–ˆìŠµë‹ˆë‹¤. Datasetsì˜ í¬ê¸°ê°€ ì‘ì„ ë•ŒëŠ” CNNì˜ ì„±ëŠ¥ì´ í›¨ì”¬ ì¢‹ì§€ë§Œ datasetsì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ViTì˜ ì„±ëŠ¥ì´ CNNì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   ![](../../assets/img/ViT/vit_10.png){: class="align-center"}

### Transfer performance

Pre-trainingì„ í•˜ëŠ”ë° í•„ìš”í•œ computational budgetê³¼ fine-tuningì„ í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ 3ê°€ì§€ë¥¼ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

- ViTëŠ” performance/compute trade-offì—ì„œ CNNì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
- ì ì€ computational budgetì—ì„œëŠ” hybrid ëª¨ë¸ì´ ì•½ê°„ ë›°ì–´ë‚˜ì§€ë§Œ, í° ViTì—ì„œëŠ” ê·¸ ì°¨ì´ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
- ViTëŠ” ì‹¤í—˜ëœ í•˜ë“œì›¨ì–´ ë²”ìœ„ ì•ˆì—ì„œ í¬í™”ë˜ì§€ ì•Šì•„ í–¥í›„ í™•ì¥ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.

![](../../assets/img/ViT/vit_11.png){: class="align-center"}

### Attention Distance

Attention distanceë€ attention weightsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ê°€ í†µí•©ë˜ëŠ” ì´ë¯¸ì§€ ê³µê°„ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ë§í•©ë‹ˆë‹¤. Attention distanceëŠ” CNNì˜ receptive fieldì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ViTì˜ ë‚®ì€ ê³„ì¸µì—ì„œ ì¼ë¶€ í—¤ë“œê°€ ì´ë¯¸ì§€ì˜ ëŒ€ë¶€ë¶„ì„ attention í•˜ê³  ìˆìŒì„ ë°œê²¬í•˜ì˜€ê³  ë‚˜ë¨¸ì§€ í—¤ë“œë“¤ì€ ì‘ì€ attention distanceë¥¼ ê°€ì§€ê³  ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ê¹Šì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ëª¨ë“  í—¤ë“œì— ëŒ€í•œ attention distanceê°€ ì¦ê°€í•˜ì˜€ê³  networkì˜ ì¶œë ¥ì¸µê³¼ ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ëŒ€ë¶€ë¶„ì˜ headë“¤ì´ token ì „ë°˜ì— ê±¸ì³ ê´‘ë²”ìœ„í•˜ê²Œ attentioní•˜ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

![](../../assets/img/ViT/vit_12.png){: class="align-center"}

## â…£. Conclusion

### Paper

- Image recognitionì— transformerë¥¼ ì§ì ‘ ì ìš©í•˜ëŠ” ë°©ë²• íƒêµ¬í–ˆìŠµë‹ˆë‹¤.
- Image-specific inductive biasesë¥¼ ì£¼ì…í•˜ì§€ ì•ŠëŠ” ëŒ€ì‹  iamge patchë¥¼ sequenceë¡œ í•´ì„í•©ë‹ˆë‹¤.
- Large datasetsì— ëŒ€í•´ pre-trainingí•  ë•Œ ë†€ë¼ìš´ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤.
- Challenges
  - ë‹¤ë¥¸ computer vision ë¶„ì•¼(detection, segmentation)ì— ViTë¥¼ ì ìš©í•˜ëŠ” ë¬¸ì œ
  - Exploring self-supervised pre-training methods
  - ViTì˜ ì¶”ê°€ì ì¸ í™•ì¥ì€ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì €ì˜ ê²°ë¡ 

ğŸ¤” Computer vision ë¶„ì•¼ì— ì˜¤ì§ self-attentionë§Œìœ¼ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì´ ì²˜ìŒ ì‚¬ìš©ëœ ë§Œí¼ ê¸°ì¡´ì— ì§€ë°°ì ì´ë˜ CNNê³¼ ë¹„êµí•˜ë ¤ëŠ” ì‹¤í—˜ì´ ì£¼ë¥¼ ì´ë¤˜ë˜ ë…¼ë¬¸ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ¤” NLPë¶„ì•¼ì™€ ë”ë¶ˆì–´ CVë¶„ì•¼ê¹Œì§€ transformerê°€ ì ìš©ë¨ì— ë”°ë¼ transformerì˜ í™•ì¥ì„±ì— ëŒ€í•´ ë‹¤ì‹œ í•œ ë²ˆ ìƒê°í•´ë³´ê²Œ ë˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

## â€» Refrence

[https://github.com/lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch)
