---
layout: post
title: "[Generative Model] DDPM : Denoising Diffusion Probabilistic Models"
author: kjy
date: 2024-01-03 16:00:00 +09:00
categories: [Deep Learning, Paper Reading]
tags: [Deep Learning, Paper Reading, Computer Vision, Generative Model]
comments: true
toc: true
math: true
---

í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [DDPM : Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) ë…¼ë¬¸ì„ í•¨ê»˜ ì½ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¬¸ì¥ë§ˆë‹¤ í•´ì„ì„ í•˜ê¸° ë³´ë‹¤ëŠ” ê° ë¬¸ë‹¨ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ìš”ì•½í•˜ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ë…¼ë¬¸ì˜ í•µì‹¬ë§Œ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ [Paper Review](https://jjjuuuun.github.io/posts/Paper-Review-DDPM/)ìœ¼ë¡œ ì´ë™í•´ì£¼ì„¸ìš”!

Emojiì˜ ì˜ë¯¸ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

ğŸ” : ë…¼ë¬¸ì— ìˆëŠ” ë‚´ìš©

ğŸ’¡ : ë…¼ë¬¸ì— ì í˜€ìˆì§€ ì•Šì€ ì‚¬ì „ì§€ì‹

ğŸ’­ : ì €ì˜ ìƒê°

â­ : ì¤‘ìš”í•œ ë‚´ìš©

âœ… : ë‚˜ì¤‘ì— í™•ì¸ì´ í•„ìš”í•œ ë‚´ìš©

## Abstract

![](../../assets/img/Paper_Reading/DDPM/ddpm_1.jpg){: .normal}

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì€ diffusion probability modelsì„ ì‚¬ìš©í•´ ë†’ì€ í€„ë¦¬í‹°ì˜ ì´ë¯¸ì§€ë¥¼ í•©ì„±í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

> ğŸ’¡ Diffusion probability models : Nonequilibrium thermodynamics(ë¹„í‰í˜• ì—´ì—­í•™)ë¡œë¶€í„° ì˜ê°ë°›ì€ latent variable modelsì˜ í•œ ì¢…ë¥˜
>
> ğŸ’¡ [Latent variable models](https://en.wikipedia.org/wiki/Latent_variable_model) : Latent variableì„ ë„ì…í•´ ë³µì¡í•œ ì‹œìŠ¤í…œì˜ ë™ì‘ì„ ëª¨ë¸ë§í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í†µê²Œì  ëª¨ë¸
>
> ğŸ’¡ [Nonequilibrium thermodynamics](https://ko.wikipedia.org/wiki/%EB%B9%84%ED%8F%89%ED%98%95%EC%97%B4%EC%97%AD%ED%95%99) : ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ì—´ì  ë° ë™ì  ì¡°ê±´ì„ ê³ ë ¤í•˜ëŠ” ì—´ì—­í•™ì˜ í•œ ë¶„ì•¼ë¡œ, ì‹œìŠ¤í…œì´ í‰í˜• ìƒíƒœê°€ ì•„ë‹ ë•Œì˜ íŠ¹ì„±ì„ ë‹¤ë£¹ë‹ˆë‹¤.

ğŸ” Weighted variational boundì— ëŒ€í•œ í•™ìŠµì„ í†µí•´ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.

> - Weighted variational bound : Diffusion probability modelsì™€ denoising score matching with Langevin dynamicsì˜ ìƒˆë¡œìš´ ì—°ê²°
>   > ğŸ’¡ Denoising score matching with Langevin dynamics : Denoising score matchingì„ í†µí•´ score functionì„ í•™ìŠµí•˜ê³  Langevin dynamcisë¥¼ í†µí•´ samplingì„ ì§„í–‰í•˜ëŠ” score-based genenerative model (âœ… ì¶”í›„ ê³µë¶€ í•„ìš”)

ğŸ” DDPMì„ autoreressive decodingì˜ ì¼ë°˜í™”ë¡œ í•´ì„ë  ìˆ˜ ìˆëŠ” progressive lossy decompression ë°©ì‹ì„ ì¸ì •í•©ë‹ˆë‹¤.

## 1. Introduction

![](../../assets/img/Paper_Reading/DDPM/ddpm_2.jpg){: .normal}

ğŸ” Generative models : `GANs`, `Autoregressive models`, `flows`, `VAEs`, `Energy-based modeling`, `Score matching`

![](../../assets/img/Paper_Reading/DDPM/ddpm_3.jpg){: .normal}

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” diffusion probabilistic models(diffusion model)ì˜ ë°œì „ì„ ë…¼í•˜ê³ ì í•©ë‹ˆë‹¤.

ğŸ” Diffusion modelì€ ìœ í•œí•œ ì‹œê°„ì´í›„ì— dataì™€ ì¼ì¹˜í•˜ëŠ” samplesì„ ìƒì„±í•˜ê¸° ìœ„í•´ `variational inference`ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ `parameterized Markov chain`ì…ë‹ˆë‹¤.

> ğŸ’¡ [Variational inference](#231-variational-inference) : ë‹¤ë£¨ê¸° ì‰¬ìš´ ë§¤ê°œë³€ìˆ˜ $\theta$ë¥¼ ì¡°ì •í•˜ì—¬ í™•ë¥ ë¶„í¬ $q(\theta)$ë¥¼ posteroir distribution $p(\theta\|x)$ì— approximation í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.
>
> ğŸ’¡ Parameterized Markov chain : Markov propertyë¥¼ ê°€ì§„ ì´ì‚°í™•ë¥ ê³¼ì •
>
> > ğŸ’¡ Markov property : íŠ¹ì • ìƒíƒœì˜ í™•ë¥ ì€ ì˜¤ì§ ê³¼ê±°ì˜ ìƒíƒœì— ì˜ì¡´

ğŸ” Reverse a diffusion process(Denoising process, Transitions of markov chain)ì„ í•™ìŠµ

> - Diffusion process(Forward process) : ì›ë˜ì˜ signal(image)ì´ íŒŒê´´ë  ë•Œê¹Œì§€ ì ì§„ì ìœ¼ë¡œ dataì— noiseë¥¼ ì¶”ê°€í•˜ëŠ” markov chain

> - Diffusion processì—ì„œì˜ noiseë¥¼ ì•„ì£¼ ì‘ì€ ì–‘ì˜ Gaussian noiseë¡œ êµ¬ì„±í•˜ë©´ denoising process ë˜í•œ conditional Gaussianìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ neural network parameterizationì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.([ì´ì „ ì—°êµ¬](https://arxiv.org/abs/1503.03585)ì—ì„œ ë°œê²¬ë¨)

![](../../assets/img/Paper_Reading/DDPM/ddpm_4.jpg){: .normal}

ğŸ” ê¸°ì¡´ diffusion model ì—°êµ¬ì˜ ì¥ì ê³¼ ë‹¨ì 

> - ì¥ì  : Diffusion modelì„ ì •ì˜í•˜ê³  í•™ìŠµí•˜ê¸° ìˆ˜ì›”
>
> - ë‹¨ì  : High quality sampleì„ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ ì˜ë¬¸

ğŸ” DDPMì—ì„œ ë³´ì—¬ì£¼ëŠ” diffusion model

> - â­ íŠ¹ì • ë§¤ê°œë³€ìˆ˜í™”($\epsilon$-prediction parameterization)ë¥¼ í†µí•´ DDPMì´ denoising score matching over multiple noise levels during <u>training</u> ê·¸ë¦¬ê³  annealed Langevin dynamics during <u>sampling</u>ê³¼ ë™ë“±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
>
> - ì´ëŸ¬í•œ íŠ¹ì • ë§¤ê°œë³€ìˆ˜í™”ë¥¼ í†µí•´ high quality sampleì„ ìƒì„±í•˜ì˜€ê³  diffusion modelì´ ì•„ë‹Œ ë‹¤ë¥¸ ìœ í˜•ì˜ generative modelë³´ë‹¤ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤.
>
> - ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ log-likelihood-based modelì— ë¹„í•´ ê²½ìŸë ¥ ìˆëŠ” ë¡œê·¸ ê°€ëŠ¥ì„±ì„ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
>
>   > Log likelihoods : Energy based models & score matching < Diffusion model < Other likelihood-based models
>
> - DDPMì˜ lossless codelengthsì˜ ëŒ€ë¶€ë¶„ì´ ê°ì§€í•  ìˆ˜ ì—†ëŠ” ì´ë¯¸ì§€ ì„¸ë¶€ ì •ë³´ë¥¼ ì„¤ëª…í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
>
>   > ğŸ’¡ Codelengths
>   >
>   > > - ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•˜ê³  ìƒì„±í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ (bits/dim)
>   > > - ì‘ì€ codelengthsëŠ” ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ëª¨ë¸ì„ì„ ëœ»í•˜ë¯€ë¡œ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ
>   >
>   > ğŸ’¡ Lossless codelength : Lossless compresorë¥¼ í†µí•´ ì¸¡ì •í•œ codelength  
>   > ğŸ’¡ Lossless compresorì˜ ëŒ€í‘œì ìœ¼ë¡œ NLL(Negativ Log Likelihood)ì´ ìˆìŒ
>
> - DDPMì˜ sampling ê³¼ì •ì´ progressive lossy decompression ë°©ì‹ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## 2. Background

### 2.1 Reverse Process

![](../../assets/img/Paper_Reading/DDPM/ddpm_5.jpg){: .normal}

ğŸ” Reverse Process : Standard normal distributionì—ì„œ í•™ìŠµ dataë¡œ denoising í•´ê°€ëŠ” ê³¼ì •ì´ë©° í•™ìŠµì˜ ëŒ€ìƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}
p_\theta(\mathbf{x}_{0:T})
&= p_\theta(\mathbf{x}_0|\mathbf{x}_{1:T})\cdot p_\theta(\mathbf{x}_{1:T}) \tag{1} \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_{1:T})\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_{2:T})\cdot p_\theta(\mathbf{x}_{2:T}) \tag{2} \\
& \qquad \vdots \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_{1:T})\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_{2:T})\cdots p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)\cdot P_\theta(\mathbf{x}_T) \tag{3} \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_1)\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_2)\cdots p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)\cdot p_\theta(\mathbf{x}_T) \tag{4} \\
&= p_\theta(\mathbf{x}_0|\mathbf{x}_1)\cdot p_\theta(\mathbf{x}_1|\mathbf{x}_2)\cdots p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)\cdot p(\mathbf{x}_T) \tag{5} \\
&= p(\mathbf{x}_T)\cdot \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) \tag{6}
\end{align}
$$

#### 2.1.1 Formula (1), (2), (3)

ë¨¼ì € í‘œê¸°ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ë©´ $p_\theta(\mathbf{x}\_{0:T})$ëŠ” ë§¤ê°œë³€ìˆ˜ $\theta, \mathbf{x}\_0, \mathbf{x}\_1, \cdots, \mathbf{x}\_T$ì˜ ê²°í•©í™•ë¥ ì—ì„œ $\theta$ë¥¼ ê³ ì •ì‹œì¼°ì„ ë•Œì˜ ì£¼ë³€í™•ë¥ ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì— ê¸°ìˆ ë˜ì–´ ìˆëŠ” ê³µì‹(chain rule)ì„ ì‚¬ìš©í•˜ë©´ `(1)`, `(2)`, `(3)`ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align} P(A, B, C) &= P(A \cap B \cap C) \\
&= P(A \cap (B \cap C)) \\
&= P(A | B \cap C)P(B \cap C) \\
&= P(A | B, C)P(B, C)
\end{align}
$$

#### 2.1.2 Formula (4)

Reverse processëŠ” markov propertyë¥¼ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì— markov processì— ì˜í•´ì„œ `(4)`ì²˜ëŸ¼ `(3)`ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 2.1.3 Formula (5)

Timestep $T$ì—ì„œì˜ $\mathbf{x}_T$ëŠ” $\theta$ì™€ ìƒê´€ì—†ì´ $\mathcal{N}(0, I)$ì´ê¸° ë•Œë¬¸ì— $\theta$ë¥¼ ì§€ì›Œ `(5)`ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 2.1.4 Formula (6)

`(5)`ë¥¼ ì¼ë°˜í™” ì‹œí‚¤ë©´ `(6)`ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.2 Forward Process (Diffusion Process)

![](../../assets/img/Paper_Reading/DDPM/ddpm_6.jpg){: .normal}

ğŸ” Forward Process(Diffusion process) : Dataì—ì„œ noiseë¥¼ ì¶”ê°€í•´ standard normal distributionìœ¼ë¡œ ê°€ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

ğŸ” ë‹¤ë¥¸ ìœ í˜•ì˜ latent variable modelsê³¼ diffusion modelì˜ ë‹¤ë¥¸ ì ì€ noiseê°€ Gaussian noiseë¡œ ê³ ì •ë˜ì–´ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

$$
\begin{align}
q(\mathbf{x}_{1:T}|\mathbf{x}_0)
&= q(\mathbf{x}_{2:T}|\mathbf{x}_0, \mathbf{x}_1)\cdot q(\mathbf{x}_1|\mathbf{x}_0) \tag{1}\\
&= q(\mathbf{x}_{3:T}|\mathbf{x}_{0:2})\cdot q(\mathbf{x}_2|\mathbf{x}_0, \mathbf{x}_1)\cdot q(\mathbf{x}_1|\mathbf{x}_0) \tag{2}\\
& \qquad \vdots \\
&= q(\mathbf{x}_T|\mathbf{x}_{0:T-1})\cdot q(\mathbf{x}_{T-1}|\mathbf{x}_{0:T-2})\cdots q(\mathbf{x}_1|\mathbf{x}_0) \tag{3}\\
&= q(\mathbf{x}_T|\mathbf{x}_{T-1})\cdot q(\mathbf{x}_{T-1}|\mathbf{x}_{T-2})\cdots q(\mathbf{x}_1|\mathbf{x}_0) \tag{4}\\
&= \prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1}) \tag{5}
\end{align}
$$

#### 2.2.1 Formula (1), (2), (3)

ì•„ë˜ì— ê¸°ìˆ ë˜ì–´ ìˆëŠ” ê³µì‹(chain rule)ì„ ì‚¬ìš©í•˜ë©´ `(1)`, `(2)`, `(3)`ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}
P(A, B, C | D)
&= P(A \cap B \cap C | D) \\
&= \frac{P(A \cap B \cap C \cap D)}{P(D)} \\
&= P(A \cap B | C \cap D)\cdot \frac{P(C \cap D)}{P(D)} \\
&= P(A, B | C, D)\cdot P(C | D) \\
\end{align}
$$

#### 2.2.2 Formula (4)

Forward processëŠ” markov propertyë¥¼ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì— markov processì— ì˜í•´ì„œ `(4)`ì²˜ëŸ¼ `(3)`ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 2.2.3 Formula (5)

`(4)`ë¥¼ ì¼ë°˜í™” ì‹œí‚¤ë©´ `(5)`ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.3 Optimization Function

![](../../assets/img/Paper_Reading/DDPM/ddpm_7.jpg){: .normal}

ğŸ” NLL(Negative Log Likelihood)ì— ëŒ€í•œ variational boundë¡œ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ğŸ’¡ Log Likelihoodë¥¼ Negative Log Likelihoodë¡œ ë³€ê²½í•œ ì´ìœ  : ìš°ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë¬¸ì œë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¡œ ë°”ê¾¸ê¸° ìœ„í•´

#### 2.3.1 Variational Inference

1. Variational transform

   - ë¹„ì„ í˜• í•¨ìˆ˜ $f(x)$ë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ ì„ í˜• í•¨ìˆ˜ $g(x)$ë¡œ ë°”ê¾¸ëŠ” ê²ƒì„ variational transformì´ë¼ í•©ë‹ˆë‹¤.
   - Variational transformì˜ í•œ ì˜ˆë¡œ $f(x) = \log$í•¨ìˆ˜ë¥¼ ì¼ì°¨í•¨ìˆ˜ë¡œ ë³€ê²½í•´ë³´ê² ìŠµë‹ˆë‹¤.
     > - $\log$ í•¨ìˆ˜ë¥¼ ì¼ì°¨í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ê²ƒì€ $\log$í•¨ìˆ˜ ìœ„ì— ìˆëŠ” ëª¨ë“  ì ì„ ì¼ì°¨í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ëœ»í•©ë‹ˆë‹¤.
     > - $\log$ í•¨ìˆ˜ì˜ ì ‘ì„ ì€ $g(x) = \lambda x + b(\lambda)$ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
     > - ì´ ë•Œ ì¼ì°¨í•¨ìˆ˜ì¸ ì ‘ì„ ì˜ ì ˆí¸ì€ ê¸°ìš¸ê¸° $\lambda$ì— ë”°ë¼ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— $b(\lambda)$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.
     > - ì ‘ì ì„ ì°¾ëŠ” ê³µì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     >
     > $$\begin{align}y = \underset{x}{\text{min}} \{g(x) - f(x)\}\end{align}$$
     >
     > - ì´ì œ ì ‘ì  êµ¬í•´ë³´ê² ìŠµë‹ˆë‹¤.(ê°„ë‹¨í•˜ê²Œ $\log$í•¨ìˆ˜ì´ ë°‘ì´ $10$ì´ë¼ ê°€ì •)
     >
     > $$\begin{align} \frac{d}{dx} \{ g(x) - f(x) \} &=& 0 \\ \frac{d}{dx} \{ \lambda x + b(\lambda) - \ln\ x\} &=& 0 \\ \lambda - \frac{1}{x} &=& 0 \\ x &=& \frac{1}{\lambda}  \end{align}$$
     >
     > - ì •ë¦¬ë¥¼ í•´ë³´ë©´ $x = \frac{1}{\lambda}$ì¸ ëª¨ë“  $x$ì¢Œí‘œì—ì„œ $\log$ í•¨ìˆ˜ë¥¼ ì¼ì°¨í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - ê·¸ëŸ¬ë‚˜ variationa transformì—ì„œ ì¤‘ìš”í•œ íŠ¹ì§•ì´ í•˜ë‚˜ ìˆìŠµë‹ˆë‹¤.
   - ê·¸ê²ƒì€ ë°”ë¡œ í•¨ìˆ˜ê°€ convexí•˜ê±°ë‚˜ concaveí•´ì•¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
     > ë§Œì•½ ë¹„ì„ í˜• í•¨ìˆ˜ $f(x)$ê°€ convexí•˜ì§€ ì•Šê³  concaveí•˜ì§€ë„ ì•Šë‹¤ë©´ ì ‘ì„  $g(x)$ì—ëŠ” ë‘ ê°œ ì´ìƒì˜ ì ‘ì ì´ ìƒê¸°ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
   - ì´ëŸ¬í•œ íŠ¹ì§• ë•Œë¬¸ì— dualityë¥¼ í†µí•´ variational tranformì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
     > - ë§Œì•½ convexí•˜ì§€ ì•Šê±°ë‚˜ concaveí•˜ì§€ ì•Šì€ í•¨ìˆ˜ê°€ ìˆë‹¤ë©´ $\log$ë¥¼ ì‚¬ìš©í•´ concaveí•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     > - Deep Learningì„ ê³µë¶€í•˜ë‹¤ë³´ë©´ $\log$ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì—ëŠ” ê³±ì„ í•©ìœ¼ë¡œ ë³€ê²½í•´ì£¼ëŠ” ì´ìœ ë„ ìˆì§€ë§Œ concaveí•œ íŠ¹ì§•ì„ ì‚¬ìš©í•´ variational transformí•˜ëŠ” ì´ìœ ë„ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
   - Variational transformì„ ì •ë¦¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
     > - Variational parameter $\lambda$ë¥¼ ë„ì…í•˜ì—¬ ë¹„ì„ í˜• í•¨ìˆ˜ì¸ $\log$ë¥¼ ì„ í˜•ìœ¼ë¡œ ë³€ê²½ í•˜ëŠ” ê²ƒì„ variational transformì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. Variational Inference

   - Variational inferenceëŠ” ë‹¤ë£¨ê¸° ì‰¬ìš´ ë§¤ê°œë³€ìˆ˜ $\theta$ë¥¼ ì¡°ì •í•˜ì—¬ í™•ë¥ ë¶„í¬ $q(\theta)$ë¥¼ posteroir distribution $p(\theta\|x)$ì— approximation í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.
   - Variational inferenceì˜ ë°©ë²•ìœ¼ë¡œëŠ” KL-Divergenceì´ ìˆìŠµë‹ˆë‹¤.

#### 2.3.2 Variational Inference at DDPM

DDPMì—ì„œ variational inferenceê°€ ì ìš©ë˜ëŠ” ë¶€ë¶„ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

$$
\begin{align}
\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p_\theta(\mathbf{x}_0)\right]

&= \int(-\log\ p_\theta(\mathbf{x}_0))\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad &  \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)}) \cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \because \text{Chain Rule} \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)} \cdot {\color{Red}\frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{ {\color{Red}q(\mathbf{x}_{1:T}|\mathbf{x}_0)}} \cdot \frac{ {\color{Red}q(\mathbf{x}_{1:T}|\mathbf{x}_0)}}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T + \int(-\log\ \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T - {\color{Blue}\int(\log\ \frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T} \qquad & \\

&= \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T - {\color{Blue}D_{KL}(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\ ||\ p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0))} \qquad &  \\

&\le \int(-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})\cdot q(\mathbf{x}_T|\mathbf{x}_0)\ d\mathbf{x}_T\ \qquad & \because D_{KL} \ge 0 \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \qquad & \\

&= ELBO \qquad & \\
\end{align}
$$

#### 2.3.3 Optimization Function êµ¬í•˜ê¸°

[Section 2.1](#21-reverse-process)ê³¼ [section 2.2](#22-forward-process-diffusion-process)ë¥¼ ì‚¬ìš©í•´ ìœ„ì—ì„œ variational inferenceë¥¼ í†µí•´ ì°¾ì€ ELBO termì„ ì•„ë˜ì™€ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}
\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p_\theta(\mathbf{x}_0)\right]

&\le \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{ {\color{Red}p(\mathbf{x}_{0:T})}}{ {\color{Blue}q(\mathbf{x}_{1:T}|\mathbf{x}_0)}}\right] \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{ {\color{Red}p(\mathbf{x}_T)\cdot \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}}{ {\color{Blue}\prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1})}}\right] \qquad & \because \text{Section 2.1}\ \&\ \text{Section2.2} \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\log\prod_{t=1}^T \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\left( \log\frac{p_\theta(\mathbf{x}_0|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_0)} + \log\frac{p_\theta(\mathbf{x}_1|\mathbf{x}_2)}{q(\mathbf{x}_2|\mathbf{x}_1)} + \cdots + \log\frac{p_\theta(\mathbf{x}_{T-1}|\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_{T-1})} \right)\right] \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=1}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \qquad & \\

&=: L \qquad & \\

\end{align}
$$

ì§€ê¸ˆê¹Œì§€ ë§ì€ ê³µì‹ë“¤ì´ ë‚˜ì™€ í—·ê°ˆë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ì§€ê¸ˆê¹Œì§€ ì¦ëª…í–ˆë˜ ê³µì‹ì„ ê°„ë‹¨íˆ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p_\theta(\mathbf{x}_0)\right] \le \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ \frac{p(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] = \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=1}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] =: L$$

### 2.4 Variance schedule $\beta\_t$

![](../../assets/img/Paper_Reading/DDPM/ddpm_8.jpg){: .normal}

ğŸ” $\beta\_t$ëŠ” reparameterizationì„ í†µí•´ í•™ìŠµë  ìˆ˜ ìˆê³  hyperparameterë¡œ ë‘ê³  í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” $\beta\_t$ê°€ ë§¤ìš° ì‘ì„ ë•Œ diffusion processì™€ reverse processëŠ” ë™ì¼í•œ í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— reverse processì˜ í‘œí˜„ì„±ì€ diffusion processì˜ gaussian conditionalsì— ì˜í•´ì„œ ë¶€ë¶„ì ìœ¼ë¡œ ë³´ì¥ë©ë‹ˆë‹¤. ($\because$ [ì´ì „ ì—°êµ¬](https://arxiv.org/abs/1503.03585))

ğŸ” Diffusion processì—ì„œ ì£¼ëª©í• ë§Œ í•œ ì ì€ diffusion processë¥¼ ì„ì˜ì˜ timestep $t$ì—ì„œ closed formìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

ğŸ” ì–´ë–»ê²Œ closed formìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ”ì§€ ì•„ë˜ ì¦ëª…ê³¼ í•¨ê»˜ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

#### 2.4.1 Reparameterziation Trick

ë”¥ëŸ¬ë‹ì—ì„œ í•„ìˆ˜ì ì¸ ê³„ì‚°ì´ë¼ í•¨ì€ ë°”ë¡œ chain ruleì— ì˜í•œ backpropagationì…ë‹ˆë‹¤.

Backpropagationì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¯¸ë¶„ì´ í•„ìˆ˜ì ì¸ë° í™•ë¥ ë³€ìˆ˜ $Z$ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•´ backpropagationì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ê·¸ë ‡ê¸° ë•Œë¬¸ì— reparameterization trickì„ ì‚¬ìš©í•´ ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë„ë¡ í•´ backpropagationì„ ì§„í–‰í•©ë‹ˆë‹¤.

ì •ë¦¬ë¥¼ í•´ë³´ë©´ í™•ë¥ ë³€ìˆ˜ $Z$ë¥¼ standard normal distributionì—ì„œ sampling í•˜ëŠ” ê³¼ì •ì´ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“¤ì–´ backpropagationì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ reparameterization trickì´ë©° ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$Z \sim \mathcal{N}(\mu, \sigma^2) \Longrightarrow Z = \mu + \sigma\cdot\epsilon (\epsilon \sim \mathcal{N}(0, \mathrm{I}))$$

#### 2.4.2 Diffusion Proecess with reprameterization trick

ë¨¼ì € diffusion proecessì˜ time step $t$ì—ì„œì˜ í™•ë¥ ë¶„í¬ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

$$
\begin{align}

& \text{Diffusion process} : &q(\mathbf{x}_t|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathrm{I}) \\ \\

\end{align}
$$

ì´ì œ reparameterization trickì„ í™œìš©í•´ timestep $t$ì—ì„œ diffusion processë¥¼ closed formìœ¼ë¡œ ë‚˜íƒ€ë‚´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

$$
\begin{align}

\mathbf{x}_t

&= \sqrt{1 - \beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\epsilon_{t-1} (\epsilon_{t-1} \sim \mathcal{N}(0, \mathrm{I})) \qquad & \because \text{Reparameterization Trick} \\

&= \sqrt{\alpha_t}{\color{Green}\mathbf{x}_{t-1}} + \sqrt{1-\alpha_t} \epsilon_{t-1} \qquad & \because \alpha_t := 1-\beta_t \\

&= \sqrt{\alpha_t}({\color{Green}\sqrt{\alpha_{t-1}}{\mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \epsilon_{t-2}}}) + \sqrt{1-\alpha_t} \epsilon_{t-1} \qquad & \\

&= \sqrt{\alpha_{t}\cdot \alpha_{t-1}}\mathbf{x}_{t-2} + {\color{Red}\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon_{t-2} + \sqrt{1-\alpha_t} \epsilon_{t-1}} \qquad & \\

&= \sqrt{\alpha_{t}\cdot \alpha_{t-1}}\mathbf{x}_{t-2} + {\color{Red}\sqrt{\alpha_t(1-\alpha_{t-1}) + 1 - \alpha_t}\bar{\epsilon}_{t-2}} \qquad & \because \epsilon_1 \sim \mathcal{N}(0, \sigma_1^2\mathrm{I})\quad \& \quad \epsilon_2 \sim \mathcal{N}(0, \sigma_2^2\mathrm{I}) \Longrightarrow \bar{\epsilon} := \epsilon_1 + \epsilon_2 \sim \mathcal{N}(0, (\sigma_1^2 + \sigma_2^2)\mathrm{I}) \\

&= \sqrt{\alpha_{t}\cdot \alpha_{t-1}}\mathbf{x}_{t-2} + {\color{Red}\sqrt{1 - \alpha_t\cdot\alpha_{t-1}}\bar{\epsilon}_{t-2}} \qquad & \\

&= \cdots \qquad & \\

&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon \qquad & \because \bar{\alpha}_t := \alpha_t\cdot\alpha_{t-1}\cdots\alpha_1 \\

\end{align}
$$

### 2.5 Rewriting $L$

![](../../assets/img/Paper_Reading/DDPM/ddpm_9.jpg){: .normal}

ğŸ” ìœ„ì—ì„œ ì •ì˜í•œ [optimization function $L$](#233-optimization-function-êµ¬í•˜ê¸°)ì„ ë³€ê²½í•¨ìœ¼ë¡œì¨ íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

$$
\begin{align}
L

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=1}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \qquad & \tag{1}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{\color{Orange}q(\mathbf{x}_t|\mathbf{x}_{t-1})} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_{0})}\right] \qquad & \because t = 1 \text{ë¶„ë¦¬} \tag{2}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{\color{Orange}q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}\cdot {\color{Orange}\frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{0})}{q(\mathbf{x}_t|\mathbf{x}_{0})}} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_{0})}\right] \qquad & \because \text{Section 2.5.1} \tag{3}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}-{\color{Purple}\sum_{t=2}^T\log \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{0})}{q(\mathbf{x}_t|\mathbf{x}_{0})}} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_{0})}\right] \qquad & \because \log\ a\cdot b = \log\ a + \log\ b \tag{4}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}-\right({\color{Purple}\log \frac{q(\mathbf{x}_{1}|\mathbf{x}_{0})}{q(\mathbf{x}_{2}|\mathbf{x}_{0})} + \log \frac{q(\mathbf{x}_{2}|\mathbf{x}_{0})}{q(\mathbf{x}_{3}|\mathbf{x}_{0})} \cdots + \log \frac{q(\mathbf{x}_{T-1}|\mathbf{x}_{0})}{q(\mathbf{x}_{T}|\mathbf{x}_{0})}}\left) - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_{0})}\right] \qquad & \tag{5}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}-{\color{Purple}\log \frac{q(\mathbf{x}_{1}|\mathbf{x}_{0})}{q(\mathbf{x}_{T}|\mathbf{x}_{0})}} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_1|\mathbf{x}_{0})}\right] \qquad & \because \log \frac{a}{b}+\log \frac{b}{c} = \log \frac{a}{c} \tag{6}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}-\log \frac{\color{Red}q(\mathbf{x}_{1}|\mathbf{x}_{0})}{q(\mathbf{x}_{T}|\mathbf{x}_{0})} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{\color{Red}q(\mathbf{x}_1|\mathbf{x}_{0})}\right] \qquad & \tag{7}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log\ p(\mathbf{x}_T) -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{\color{Blue}q(\mathbf{x}_{T}|\mathbf{x}_{0})}\right] \qquad & \because \log \frac{a}{b}+\log \frac{b}{c} = \log \frac{a}{c} \tag{8}\\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log \frac{p(\mathbf{x}_T)}{\color{Blue}q(\mathbf{x}_{T}|\mathbf{x}_{0})} -\sum_{t=2}^T\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})} - \log\ p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)\right] \qquad & \tag{9}\\

&= {\color{Red}\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_{T}|\mathbf{x}_{0})}\right]} + \sum_{t=2}^T{\color{Green}\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}\right]} + {\color{Blue}\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[- \log\ p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)\right]} \qquad & \tag{10}\\

&= {\color{Red}D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0)||p(\mathbf{x}_T))} + \sum_{t=2}^T {\color{Green}D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0}) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))} + {\color{Blue}\mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[- \log\ p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)\right]} \qquad & \because D_{KL}(Q|P) = \mathbb{E}_{\mathbf{x}\ \sim\ Q(\mathbf{x})}\left[-\log \frac{P(\mathbf{x})}{Q(\mathbf{x})}\right] = - \sum Q(\mathbf{x})\left(\log \frac{P(\mathbf{x})}{Q(\mathbf{x})}\right) \qquad \tag{11}\\

&= {\color{Red}L_T} + \sum_{t=2}^T {\color{Green}L_{t-1}} + {\color{Blue}L_0} \qquad & \tag{12}\\

&= {\color{Red}L_T} + {\color{Green}L_{1:T-1}} + {\color{Blue}L_0} \qquad & \tag{13}\\

\end{align}
$$

#### 2.5.1 Formula (2), (3) : Rewriting $q(\mathbf{x}\_t|\mathbf{x}\_{t-1})$

ğŸ” $q(\mathbf{x}\_{t}\|\mathbf{x}\_{t-1})$ì— conditionìœ¼ë¡œ $\mathbf{x}\_{0}$ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ë‹¤ë£¨ê¸° ì‰½ë„ë¡ í•´ KL divergence(${\color{Green} L\_{t-1}}$)ë¥¼ í†µí•´ ì§ì ‘ì ìœ¼ë¡œ $p\_{\theta}(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t})$ì™€ ë¹„êµí•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.

$$
\begin{align}

{\color{Orange}q(\mathbf{x}_t|\mathbf{x}_{t-1})}

&= q(\mathbf{x}_t|\mathbf{x}_{t-1}, {\color{Orange}\mathbf{x}_{0}}) \qquad & \\

&= \frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}, \mathbf{x}_{0})}{q(\mathbf{x}_{t-1}, \mathbf{x}_{0})} \qquad & \\

&= \frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}, \mathbf{x}_{0})}{\color{Red}q(\mathbf{x}_{t-1}, \mathbf{x}_{0})}\cdot \frac{q(\mathbf{x}_t, \mathbf{x}_{0})}{\color{Purple}q(\mathbf{x}_t, \mathbf{x}_{0})} \qquad & \\

&= \frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}, \mathbf{x}_{0})}{\color{Purple}q(\mathbf{x}_t, \mathbf{x}_{0})}\cdot \frac{\color{Blue}q(\mathbf{x}_t, \mathbf{x}_{0})}{\color{Red}q(\mathbf{x}_{t-1}, \mathbf{x}_{0})}  \qquad & \\

&= \frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}, \mathbf{x}_{0})}{q(\mathbf{x}_t, \mathbf{x}_{0})}\cdot \frac{\color{Blue}q(\mathbf{x}_t|\mathbf{x}_{0})\cdot q(\mathbf{x}_{0})}{\color{Red}q(\mathbf{x}_{t-1}|\mathbf{x}_{0})\cdot q(\mathbf{x}_{0})} \qquad & \\

&= {\color{Green}\frac{q(\mathbf{x}_t, \mathbf{x}_{t-1}, \mathbf{x}_{0})}{q(\mathbf{x}_t, \mathbf{x}_{0})}}\cdot \frac{\color{Blue}q(\mathbf{x}_t|\mathbf{x}_{0})}{\color{Red}q(\mathbf{x}_{t-1}|\mathbf{x}_{0})} \qquad & \\

&= {\color{Green}q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}\cdot \frac{q(\mathbf{x}_t|\mathbf{x}_{0})}{q(\mathbf{x}_{t-1}|\mathbf{x}_{0})} \qquad & \\

\end{align}
$$

### 2.6 What probability distribution $q(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ follows

![](../../assets/img/Paper_Reading/DDPM/ddpm_10.jpg){: .normal}

$$
\begin{align}

q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_{0})

&= {\color{Red}q(\mathbf{x}_t|\mathbf{x}_{t-1})}\cdot\frac{\color{Blue}q(\mathbf{x}_{t-1}|\mathbf{x}_{0})}{\color{Green}q(\mathbf{x}_{t}|\mathbf{x}_{0})} \qquad & \because \text{Section 2.5.1}\\

&= {\color{Red}\frac{1}{\sqrt{2\pi\beta_{t}}}\exp\left(-\frac{(\mathbf{x}_{t}-\sqrt{1-\beta_{t}}\mathbf{x}_{t-1})^2}{2\beta_{t}}\right)} \cdot {\color{Blue}\frac{1}{\sqrt{2\pi(1-\bar{\alpha}_{t-1})}}\exp\left(-\frac{(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_{0})^2}{2(1-\bar{\alpha}_{t-1})}\right)} \cdot {\color{Green}\sqrt{2\pi(1-\bar{\alpha}_{t})}\exp\left(\frac{(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0})^2}{2(1-\bar{\alpha}_{t})}\right)}\qquad & \because x \sim \mathcal{N}(\mu, \sigma^2) \Longrightarrow f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \\

&\propto \exp\left(-\frac{(\mathbf{x}_{t}-\sqrt{1-\beta_{t}}\mathbf{x}_{t-1})^2}{2\beta_{t}}\right) \cdot \exp\left(-\frac{(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_{0})^2}{2(1-\bar{\alpha}_{t-1})}\right) \cdot \exp\left(\frac{(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0})^2}{2(1-\bar{\alpha}_{t})}\right) \qquad & \because \text{ìƒìˆ˜ ì œê±°} \\

&= \exp\left(-\frac{1}{2}\frac{(\mathbf{x}_{t}-\sqrt{1-\beta_{t}}{\color{Orange}\mathbf{x}_{t-1}})^2}{\beta_{t}} + \frac{({\color{Orange}\mathbf{x}_{t-1}}-\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_{0})^2}{(1-\bar{\alpha}_{t-1})} - \frac{(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0})^2}{(1-\bar{\alpha}_{t})}\right) \qquad & \because \exp\ a \cdot \exp\ b = \exp (a+b) \\

&= \exp\bigg(-\frac{1}{2}\Big({\color{Red}(\frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1-\bar{\alpha}_{t-1}})}{\color{Orange}\mathbf{x}_{t-1}^2} - {\color{Blue} (\frac{2\sqrt{\alpha_{t}}}{\beta_{t}}\mathbf{x}_{t} + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}}\mathbf{x}_{0})}{\color{Orange}\mathbf{x}_{t-1}} + C(\mathbf{x}_{t}, \mathbf{x}_{0})\Big) \bigg)  \qquad & \because \text{Conditionìœ¼ë¡œ }\mathbf{x}_{t}\text{ì™€ }\mathbf{x}_{0}\text{ê°€ ì£¼ì–´ì¡Œì„ ë•Œ }\mathbf{x}_{t-1}\text{ì˜ í™•ë¥ ë¶„í¬ë¥¼ ì°¾ê³ ì í•¨} \\

&= \exp\bigg(-\frac{1}{2}\Big({\color{Red}a}\mathbf{x}_{t-1}^2 - {\color{Blue}b}\mathbf{x}_{t-1} + C(\mathbf{x}_{t}, \mathbf{x}_{0})\Big)\bigg) \qquad & \\

&= \exp\bigg(-\frac{\color{Red}a}{2}\Big(\mathbf{x}_{t-1} - \frac{\color{Blue}b}{\color{Red}a}\Big)^2 \bigg) + C^\prime(\mathbf{x}_{t}, \mathbf{x}_{0}) \qquad & \\

&\propto \frac{1}{\sqrt{2\pi\frac{1}{\color{Red}a}}}\exp\left(-\frac{(\mathbf{x}_{t-1}-\frac{\color{Blue}b}{\color{Red}a})^2}{2\frac{1}{\color{Red}a}}\right) + C^{''}(\mathbf{x}_{t}, \mathbf{x}_{0}) \qquad & \\

\end{align}
$$

ìœ„ì˜ ì‹ $q(\mathbf{x}\_{t-1}\|\mathbf{x}\_{t}, \mathbf{x}\_{0})$ì„ ì •ë¦¬í•´ë³´ë©´ í‰ê· ì´ ${\color{Blue}b}/{\color{Red}a}$ì´ê³  ë¶„ì‚°ì´ $1/{\color{Red}a}$ì¸ gaussian distributionì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì— ìˆëŠ” ì‹ìœ¼ë¡œ ì •ë¦¬ë¥¼ í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}

&q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_{t-1};\tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t\mathrm{I}) \\

&\text{where}\quad \tilde{\beta} := \frac{1}{\color{Red}a} = \frac{1 - \bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t\quad \text{and}\quad

\tilde{\mu}_t(\mathbf{x}_{t}, \mathbf{x}_{0}) := {\color{Blue}b}/{\color{Red}a} = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t

\end{align}
$$

ğŸ” ê²°ê³¼ì ìœ¼ë¡œ optimization function $L$ì˜ ëª¨ë“  $D_{KL}$ì€ gaussian distribution ë¹„êµì´ë¯€ë¡œ high variance Monte Carlo estimates ëŒ€ì‹  closed formì„ ì‚¬ìš©í•˜ì—¬ Rao-Blackwellized ë°©ì‹ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 3. Diffusion models and denoising autoencoders

![](../../assets/img/Paper_Reading/DDPM/ddpm_11.jpg){: .normal}

ğŸ” Diffusion models : ì œì•½ì´ ìˆëŠ” latent variable modelsì˜ í•œ ì¢…ë¥˜ë¡œ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ êµ¬í˜„ì—ì„œ <u>ë§ì€ ììœ ë„</u>ë¥¼ í—ˆìš©í•©ë‹ˆë‹¤.

> Forward process(Diffusion process)ì˜ variances $\beta\_t$([Section 3.1](#31-forward-process-and-l_t))
>
> Reverse processì˜ Gaussian distribution parameterization([Section 3.2, 3.3, 3.4](#32-reverse-process-and-l_1t-1))
>
> Model Architecture([Section 4](#4-experiments))

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì˜ ì„ íƒì„ ì•ˆë‚´í•˜ê¸° ìœ„í•´ diffusion modelsê³¼ denoising score matching ì‚¬ì´ì˜ ìƒˆë¡œìš´ ì—°ê²°ì„ í™•ë¦½í•˜ê³  ì´ ìƒˆë¡œìš´ ì—°ê²°ì€ ë‹¨ìˆœí•˜ê³  ê°€ì¤‘í™”ëœ variational bound objectiveë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.

ğŸ” DDPMì€ ë‹¨ìˆœì„±ê³¼ ê²½í—˜ì  ê²°ê³¼ì— ì˜í•´ ì •ë‹¹í™”ë©ë‹ˆë‹¤.

### 3.1 Forward process and $L\_T$

![](../../assets/img/Paper_Reading/DDPM/ddpm_12.jpg){: .normal}

ğŸ” [Section 2.5](#25-rewriting-l)ì—ì„œì˜ $L\_T$ë¥¼ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\begin{align}

L_T

&= D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0)||p(\mathbf{x}_T)) \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_{T}|\mathbf{x}_{0})}\right] \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log \frac{\epsilon}{\sqrt{\bar{\alpha}_T}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_T}\epsilon}\right] \qquad & \because p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T; \mathrm{0}, \mathrm{I})\quad \& \quad q(\mathbf{x}_{T}|\mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_T; \sqrt{\bar{\alpha}_T}\mathbf{x}_0, (1-\bar{\alpha}_T)\mathrm{I})\\

\end{align}
$$

ğŸ” $\beta\_t$ê°€ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì„ì„ ë¬´ì‹œí•˜ê³  ìƒìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ğŸ” ë”°ë¼ì„œ $q$ëŠ” í•™ìŠµ íŒŒë¦¬ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.

â­ ê·¸ë ‡ê¸° ë•Œë¬¸ì— $L\_T$ëŠ” í•™ìŠµí•˜ì§€ ì•Šê³  ë¬´ì‹œí•©ë‹ˆë‹¤.

### 3.2 Reverse process and $L\_{1:T-1}$

![](../../assets/img/Paper_Reading/DDPM/ddpm_13.jpg){: .normal}

ğŸ” [Section 2.5](#25-rewriting-l)ì—ì„œì˜ $L\_{t-1}$ë¥¼ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\begin{align}

L_{t-1}

&= D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0}) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[-\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_{0})}\right] \qquad & \\

\end{align}
$$

#### 3.2.1 $\Sigma\_\theta(\mathbf{x}\_t, t)$

ğŸ” ë¨¼ì € $p\_\theta(\mathbf{x}\_{t-1}\|\mathbf{x}\_t) = \mathcal{N}(\mathbf{x}\_{t-1}; \mu\_\theta(\mathbf{x}\_t, t), \Sigma\_\theta(\mathbf{x}\_t, t))$ for $1 < t \le T$ì— ëŒ€í•´ì„œ ë…¼í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

> DDPMì—ì„œëŠ” reverse processì˜ ë¶„í¬ë¥¼ Gaussian distributionìœ¼ë¡œ ì„¤ì •í–ˆê³  í•´ë‹¹ ë¶„í¬ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ $\mu\_\theta(\mathbf{x}\_t, t), \Sigma\_\theta(\mathbf{x}\_t, t)$ë¡œ ì •ì˜ë¥¼ í–ˆìŠµë‹ˆë‹¤.

ğŸ” í•´ë‹¹ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ë¨¼ì € $\Sigma\_\theta(\mathbf{x}\_t, t)$ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” time dependent constantsì¸ $\sigma\_t^2\mathrm{I}$ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

ğŸ” ì´ë•Œ ì‹¤í—˜ì ìœ¼ë¡œ `1`ë²ˆ : $\sigma\_t^2 = \beta\_t$ê³¼ `2`ë²ˆ : $\sigma\_t^2 = \tilde{\beta}\_t = \frac{1 - \bar{\alpha}\_{t-1}}{1-\bar{\alpha}\_t}\cdot\beta\_t \cdots 2$ì˜ ê²°ê³¼ê°€ ë¹„ìŠ·í•¨ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

ğŸ” `1`ë²ˆì˜ ê²½ìš° ëª¨ë“  tì—ì„œ ë…ë¦½ì ì´ë¯€ë¡œ $\mathbf{x}\_0 \sim \mathcal{N}(\mathrm{0}, \mathrm{I})$ì˜ ê²½ìš° ìµœì ì´ ë©ë‹ˆë‹¤.

ğŸ” `2`ë²ˆì˜ ê²½ìš° ì´ì „ tì— ëŒ€í•´ì„œ ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ($\bar{\alpha}\_t$) ê²°ì •ë¡ ì ìœ¼ë¡œ í•œ ì ìœ¼ë¡œ ì„¤ì •ëœ ê²½ìš° ìµœì ì´ ë©ë‹ˆë‹¤.

ğŸ” `1`ë²ˆê³¼ `2`ë²ˆì€ reverse processì— ëŒ€í•´ ê°ê° upper boundì™€ lower boundì— í•´ë‹¹í•©ë‹ˆë‹¤.([ì´ì „ ì—°êµ¬](https://arxiv.org/abs/1503.03585))

#### 3.2.2 $\mu\_\theta(\mathbf{x}\_t, t)$

![](../../assets/img/Paper_Reading/DDPM/ddpm_14.jpg){: .normal}

ğŸ” $\sigma\_t^2 = \tilde{\beta}\_t$ì„ í™œìš©í•´ì„œ ì•Œê³  ìˆëŠ” ì •ë³´ë“¤ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \sigma_t^2\mathrm{I}) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \tilde{\beta}_t\mathrm{I})$$

$$q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_{t-1};\tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t\mathrm{I})$$

ğŸ” KL-Divergenceì˜ ë¹„êµ ëŒ€ìƒì¸ ë‘ ë¶„í¬ê°€ ë¶„ì‚°ì´ ê°™ì€ Gaussian distributionì´ë¯€ë¡œ $L\_{t - 1}$ì„ ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ë‹¤ì‹œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}

L_{t-1} = \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\lVert\tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t)\rVert^2\right] + C& \\

\text{where C is a constant that does not depend on }\theta&

\end{align}
$$

ğŸ” ì¦‰, $\mu\_\theta$ì˜ ê°€ì¥ ê°„ë‹¨í•œ parameterizationì€ forward process posterior meanì¸ $\tilde{\mu}\_\theta$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” ìœ„ì—ì„œ ë‹¤ì‹œ ì •ì˜í•œ $L\_{t-1}$ì„ reparameterization trickì„ ì‚¬ìš©í•´ ë³€ê²½í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\begin{align}

L_{1:T-1}

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\lVert\tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t)\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \because \tilde{\mu}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{ {\color{Red}\sqrt{\bar{\alpha}_{t-1}}} \beta_t}{1-\bar{\alpha}_t}\cdot\frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\epsilon}{\color{Red}\sqrt{\bar{\alpha}_t}} + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \because \mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon \Longrightarrow \mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}  \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{\beta_t}{1-\bar{\alpha}_t}\cdot\frac{ {\color{Blue}\mathbf{x}_t} - \sqrt{1 - \bar{\alpha}_t}\epsilon}{\color{Red}\sqrt{\alpha_t}} + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}{\color{Blue}\mathbf{x}_t} - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\left(\frac{\beta_t}{(1-\bar{\alpha}_t) \cdot \sqrt{\alpha_t}} + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\right){\color{Blue}\mathbf{x}_t} - \frac{\beta_t\cdot {\color{Green}\sqrt{1 - \bar{\alpha}_t}}\epsilon}{ {\color{Green}(1-\bar{\alpha}_t)} \cdot \sqrt{\alpha_t}} - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\left(\frac{\beta_t}{(1-\bar{\alpha}_t) \cdot {\color{Orange}\sqrt{\alpha_t}}} + \frac{ {\color{Orange}\sqrt{\alpha_t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\right)\mathbf{x}_t - \frac{\beta_t\cdot\epsilon}{ {\color{Green}\sqrt{1 - \bar{\alpha}_t}} \cdot {\color{Orange}\sqrt{\alpha_t}}} - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ {\color{Orange}\sqrt{\alpha_t}}}\Bigg(\bigg(\frac{ {\color{Purple}\beta_t}}{1-\bar{\alpha}_t} + \frac{ {\color{Orange}\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\bigg)\mathbf{x}_t - \frac{\beta_t\cdot\epsilon}{ \sqrt{1 - \bar{\alpha}_t}}\Bigg) - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ \sqrt{\alpha_t}}\Bigg(\bigg(\frac{ {\color{Purple}1-\alpha_t}}{1-\bar{\alpha}_t} + \frac{ {\color{Brown}\alpha_t(1-\bar{\alpha}_{t-1})}}{1-\bar{\alpha}_t}\bigg)\mathbf{x}_t - \frac{\beta_t\cdot\epsilon}{ \sqrt{1 - \bar{\alpha}_t}}\Bigg) - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ \sqrt{\alpha_t}}\Bigg(\bigg(\frac{ 1-\alpha_t}{1-\bar{\alpha}_t} + \frac{ {\color{Brown}\alpha_t-\bar{\alpha}_{t}}}{1-\bar{\alpha}_t}\bigg)\mathbf{x}_t - \frac{\beta_t\cdot\epsilon}{ \sqrt{1 - \bar{\alpha}_t}}\Bigg) - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ \sqrt{\alpha_t}}\Bigg(\mathbf{x}_t - \frac{\beta_t}{ \sqrt{1 - \bar{\alpha}_t}}\cdot\epsilon\Bigg) - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] + C \qquad & \\ &\propto \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ \sqrt{\alpha_t}}\Bigg(\mathbf{x}_t - \frac{\beta_t}{ \sqrt{1 - \bar{\alpha}_t}}\cdot\epsilon\Bigg) - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] \qquad & \\

\end{align}
$$

ğŸ” ë³€ê²½ëœ $L\_{t-1}$ì—ì„œ $\beta\_t$ê°€ ìƒìˆ˜ì´ê³  $\mathbf{x}\_t$ê°€ ì£¼ì–´ì§„ ê°’ì´ê¸° ë•Œë¬¸ì— $\epsilon$ì„ ì˜ˆì¸¡í•˜ëŠ” function approximatorì¸ $\epsilon\_\theta$ë¥¼ í†µí•´ ìµœì†Œí™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” ì¦‰ ì•„ë˜ì™€ ê°™ì´ $L\_{t-1}$ì„ ë‹¤ì‹œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}

L_{t-1}

&\propto \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ \sqrt{\alpha_t}}\Bigg(\mathbf{x}_t - \frac{\beta_t}{ \sqrt{1 - \bar{\alpha}_t}}\cdot\epsilon\Bigg) - \mu_\theta(\mathbf{x}_t, t)\Bigg\rVert^2\right] \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{1}{2\sigma_t^2}\Bigg\lVert\frac{1}{ \sqrt{\alpha_t}}\Bigg(\mathbf{x}_t - \frac{\beta_t}{ \sqrt{1 - \bar{\alpha}_t}}\cdot\epsilon\Bigg) - \frac{1}{ \sqrt{\alpha_t}}\Bigg(\mathbf{x}_t - \frac{\beta_t}{ \sqrt{1 - \bar{\alpha}_t}}\cdot\epsilon_\theta\Bigg)\Bigg\rVert^2\right] \qquad & \\

&= \mathbb{E}_{\mathbf{x}_T\ \sim\ q(\mathbf{x}_T|\mathbf{x}_0)}\left[\frac{\beta_t^2}{2\sigma_t^2\cdot\alpha_t\cdot(1-\bar{\alpha}_t)}\lVert\epsilon - \epsilon_\theta\rVert^2\right] \qquad & \\

\end{align}
$$

ğŸ” ì´ë¥¼ í†µí•´ Diffusion probability modelsì™€ denoising score matching with Langevin dynamicsì˜ ìƒˆë¡œìš´ ì—°ê²°ì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

> ğŸ” Sampling ì ˆì°¨(Algorithm 2 - denoising score mtaching)ëŠ” data densityì˜ í•™ìŠµëœ ê¸°ìš¸ê¸°ë¡œì„œ $\epsilon\_\theta$ì„ ê°–ëŠ” Langevin dynamics(Variational bound for the Langevin-like reverse process)ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.

![](../../assets/img/Paper_Reading/DDPM/ddpm_15.jpg){: .normal}

ğŸ” ìš”ì•½í•˜ìë©´ reverse process mean function approximator $\mu\_\theta$ë¥¼ í›ˆë ¨í•˜ì—¬ $\tilde{\mu}\_t$ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìœ¼ë©° $\epsilon$ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” $\epsilon$ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•œ $\epsilon$-prediction parameterizationì´ Langevin dynamicsì™€ ìœ ì‚¬í•œ ê²ƒì„ ë³´ì—¬ì¤¬ê³  diffusion modelì˜ variational boundë¥¼ denoising score matchingê³¼ ìœ ì‚¬í•œ objectiveë¡œ ë‹¨ìˆœí™”í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤.

ğŸ” ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  $p\_\theta(\mathbf{x}\_{t-1}\|\mathbf{x}\_t)$ì˜ ë˜ ë‹¤ë¥¸ parameterizationì¼ ë¿ì´ê¸° ë•Œë¬¸ì— $\epsilon$-predictionê³¼ $\tilde{\mu}\_t$-predictionì„ ë¹„êµí•´ $\epsilon$-predictionì˜ íš¨ê³¼ë¥¼ Section 4ì—ì„œ ablationì„ í†µí•´ ê²€ì¦í•©ë‹ˆë‹¤.

### 3.3 Data scaling, reverse process decoder, and $L\_0$

![](../../assets/img/Paper_Reading/DDPM/ddpm_16.jpg){: .normal}

ğŸ” ì´ë¯¸ì§€ì˜ êµ¬ì„± ì›ì†Œ $\{0, 1, \dots, 255\}$ì˜ ë²”ìœ„ë¥¼ $[-1, 1]$ë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ reverse processê°€ ì¼ê´€ëœ ìŠ¤ì¼€ì¼ì˜ ì…ë ¥ì—ì„œ ì‘ë™í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

ğŸ” Discrete log likelihhodë¥¼ ì–»ê¸° ìœ„í•´ reverse processì˜ ë§ˆì§€ë§‰ ê³¼ì •ì„ Gaussian distirubtion($\mathcal{N}(\mathbf{x}\_{0}; \mu\_\theta(\mathbf{x}\_1, 1), \sigma\_1^2\mathrm{I})$)ì—ì„œ íŒŒìƒëœ ë…ë¦½ì ì¸ discrete decoderë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

ğŸ” ì´ëŸ¬í•œ discrete log likelihoodëŠ” variational boundê°€ discrete dataì— ëŒ€í•´ lossless codelengthì„ ë³´ì¥í•©ë‹ˆë‹¤.

ğŸ” Samplingì´ ëë‚˜ë©´ $\mu\_\theta(\mathbf{x}\_1, 1)$ë¥¼ noise ì—†ì´ í‘œì‹œí•©ë‹ˆë‹¤.

### 3.4 Simplified training objective

![](../../assets/img/Paper_Reading/DDPM/ddpm_17.jpg){: .normal}

ğŸ” ê²°ê³¼ì ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì€ $L_{t-1}$ê³¼ $L_0$ì„ì„ ì•Œ ìˆ˜ ìˆëŠ”ë° ë‘ objectiveë¥¼ í•©ì³ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\begin{align}

L_{\text{simple}}(\theta)

&:= \mathbb{E}\left[\lVert\epsilon - \epsilon_\theta\rVert^2\right] \\

&\text{where } t \text{ is uniform between } 1 \text{and } T

\end{align}
$$

ğŸ” $t = 1$ì¸ ê²½ìš° $L\_0$ì— í•´ë‹¹í•˜ê³  Gaussian probability density functionì„ approximationí•˜ëŠ” discrete decoderë¥¼ integralí•œ ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” $t > 1$ì¸ ê²½ìš° $L_\{t-1}$ì—ì„œ ì•ì˜ ìƒìˆ˜ë¥¼ ì œê±°í•œ ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” $L_\{t-1}$ì—ì„œ ì•ì˜ ìƒìˆ˜ë¥¼ ì œê±°í•˜ê¸° ë•Œë¬¸ì— denoising ì‘ì—…ì— ë” ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ networkë¥¼ í›ˆë ¨ ì‹œí‚¤ëŠ” ê²ƒì„ Section 4ì—ì„œ ì‹¤í—˜ì„ í†µí•´ ë³´ì—¬ì¤ë‹ˆë‹¤.

## 4. Experiments

![](../../assets/img/Paper_Reading/DDPM/ddpm_18.jpg){: .normal}

ğŸ” Time step $T = 1000$

ğŸ” Forward process variances to constants increasing linearly ($\beta\_t = 10^{-4} \sim 0.02$)

ğŸ” $\beta\_t$ëŠ” $[-1, 1]$ë¡œ scalingëœ dataì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ê²Œ ì„ íƒë˜ì–´ reverse processì™€ forward processê°€ ê±°ì˜ ë™ì¼í•œ ê¸°ëŠ¥ í˜•íƒœë¥¼ ê°€ì§€ë©´ì„œ signal-to-noise ratioë¥¼ ê°€ëŠ¥í•œ ì‘ê²Œ $\mathbf{x}\_T$ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ğŸ’­ Signal-to-noise ratioë¥¼ ê°€ëŠ¥í•œ ì‘ê²Œ $\mathbf{x}\_T$ë¡œ ìœ ì§€í•œë‹¤ëŠ” ê²ƒì€ ì›ë˜ì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ì •ë„ë¡œ noiseê°€ ì˜ ì¶”ê°€ëœ ê²ƒì„ ì˜ë¯¸í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ” U-Net backbone (similar to an unmasked PixelCNN++) with group normalization ì‚¬ìš©

ğŸ” Transformer sinusoidal position embeddingì„ ì‚¬ìš©í•´ì„œ parameterë¥¼ ê³µìœ 

ğŸ” $16 \times 16$ feature map resolutionì—ì„œ self-attetion ì‚¬ìš©

### 4.1 Sample quality

![](../../assets/img/Paper_Reading/DDPM/ddpm_19.jpg){: .normal}

### 4.2 Reverse process parameterization and training objective ablation

![](../../assets/img/Paper_Reading/DDPM/ddpm_20.jpg){: .normal}

ğŸ” $\tilde{\mu}$-predictionì€ $L$ì„ ë‹¨ìˆœí™” í•˜ê¸° ì „ì—ì„œë§Œ ì˜ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬

ğŸ” $\Sigma\_\theta(\mathbf{x}\_t, t)$ë¥¼ ìƒìˆ˜ë¡œ ê³ ì •ì‹œí‚¤ì§€ ì•Šê³  í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš° í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ë©° sample qualityê°€ ì €í•˜ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

> ğŸ’¡ ìƒìˆ˜ë¡œ ê³ ì • ì‹œí‚¨ ê²½ìš° notation : fixed isotropic $\Sigma$
>
> ğŸ’¡ í•™ìŠµ ëŒ€ìƒì¸ ê²½ìš° notation : learned diagonal $\Sigma$

ğŸ” $\epsilon$-predictionì€ $l\_{\text{simple}}$ë¡œ í•™ìŠµì„ ì§„í–‰í•  ë•Œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.

### 4.3 Progressive coding

![](../../assets/img/Paper_Reading/DDPM/ddpm_21.jpg){: .normal}

ğŸ” Trainê³¼ test ì‚¬ì´ì— ìµœëŒ€ $0.03$bits/dim ì°¨ì´ê°€ ìˆì§€ë§Œ ë‹¤ë¥¸ likelihood-based modelê³¼ ë¹„ìŠ·í•œ ì°¨ì´ì´ë©° ì´ëŠ” diffusion modelì´ overfitting ë˜ì§€ ì•Šì•˜ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” Diffusion (original)ë³´ë‹¤ëŠ” ë‚®ì€ lossless codelengthsë¥¼ ê°€ì§€ì§€ë§Œ ë‹¤ë¥¸ likelihood-based modelë³´ë‹¤ëŠ” ì—¬ì „íˆ ë†’ì€ lossless codelengthsë¥¼ ê°€ì§‘ë‹ˆë‹¤.

ğŸ” ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ë¦¬ì˜ sampleì€ high qualityì´ê¸° ë•Œë¬¸ì— diffusion modelì€ í›Œë¥­í•œ lossy compressorsë¥¼ ë§Œë“œëŠ” inductive biasë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ì €ìë“¤ì€ ê²°ë¡œ ì§€ì—ˆìŠµë‹ˆë‹¤.

ğŸ” $L\_1 +\ \cdots\ + L\_T$ë¥¼ rateë¡œ, $L\_0$ë¥¼ distortionìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ CIFAR10 modelì€ $1.78$bits/dimì˜ rateì™€ $1.97$bits/dimì˜ distortionì„ ê°€ì§€ë©°, ì´ëŠ” $0$ì—ì„œ $255$ê¹Œì§€ì˜ ì²™ë„ì—ì„œ $0.95$(MSE)ì— í•´ë‹¹í•©ë‹ˆë‹¤.

ğŸ” ë˜í•œ distortionì´ lossless codelengthsì˜ ì ˆë°˜ ì´ìƒì„ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.($1.97 / 3.75$)

#### 4.3.1 Progressive lossy compression

![](../../assets/img/Paper_Reading/DDPM/ddpm_22.jpg){: .normal}

ğŸ” Progressive lossy codeë¥¼ í†µí•´ rate-distortionì— ëŒ€í•´ ë” ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.

ğŸ” ReceiverëŠ” ì–´ëŠ $t$ì—ì„œë“ ì§€ ë¶€ë¶„ ì •ë³´ë¥¼ ì™„ì „íˆ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©° ì ì§„ì ìœ¼ë¡œ ì¶”ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

> ë…¼ë¬¸ì˜ ê³µì‹ 4ë²ˆì—ì„œ $\mathbf{x}\_0$ë¡œ ì‹ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>
> > $\mathbf{x}\_0 \approx \hat{\mathbf{x}}\_0 = (\mathbf{x}\_t - \sqrt{1 - \bar{\alpha}\_t}\epsilon) / \sqrt{\bar{\alpha}\_t}$

ğŸ” Distortion(RMSE) : $\sqrt{\lVert\mathbf{x}\_0 - \hat{\mathbf{x}}\_0\rVert^2 / D}$

> ğŸ” Rate-distortion plotì—ì„œ rateê°€ ì‘ì€ ë¶€ë¶„ì—ì„œ distortionì´ ë§¤ìš° ê¸‰ê²©í•˜ê²Œ ì¤„ì–´ë“¤ì—ˆëŠ”ë° ì´ê²ƒì€ ë¹„íŠ¸ì˜ ëŒ€ë¶€ë¶„ì´ ì‹¤ì œë¡œ ê°ì§€í•  ìˆ˜ ì—†ëŠ” distortionì— í• ë‹¹ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ğŸ” Rate : Cumulative number of bits received so far at time $t$

#### 4.3.2 Progressive generation

![](../../assets/img/Paper_Reading/DDPM/ddpm_23.jpg){: .normal}

ğŸ” Objectì˜ í˜•ì²´ê°€ ë¨¼ì € ë‚˜íƒ€ë‚˜ê³  ì´í›„ detail ì •ë³´ë“¤ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

ğŸ” ì´ê²ƒë“¤ì´ conceptual compressionì˜ íŒíŠ¸ì¼ ê²ƒì…ë‹ˆë‹¤.

#### 4.3.3 Connection to autoregressive decoding

![](../../assets/img/Paper_Reading/DDPM/ddpm_24.jpg){: .normal}

ğŸ” ë…¼ë¬¸ì˜ ê³µì‹ 5ë²ˆì˜ $L\_{t-1}$ì„ í†µí•´ Gaussian diffusion modelì„ ì¢Œí‘œ ìˆœì„œë¥¼ reorderingí•œ ì¼ì¢…ì˜ autoregressive modelë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” Noiseë¥¼ ì´ë¯¸ì§€ì— ì¶”ê°€í•  ë•Œ Gaussian noiseê°€ masking noiseë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê¸° ë•Œë¬¸ì— ë” ë‚˜ì€ íš¨ê³¼ë¥¼ ë³´ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ” ë˜í•œ Gaussian diffusion lengthëŠ” ë°ì´í„° ì°¨ì›ê³¼ ë™ì¼í•˜ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ ë¹ ë¥¸ samplingì„ ìœ„í•´ ë” ì§§ê²Œ ë˜ëŠ” ëª¨ë¸ í‘œí˜„ì„±ì„ ìœ„í•´ ë” ê¸¸ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ğŸ’­ Autoregressive modelì˜ ê²½ìš° ë°ì´í„° ì°¨ì›ê³¼ ë™ì¼í•  ìˆ˜ ë°–ì— ì—†ê¸° ë•Œë¬¸ì— Gaussian diffusionì˜ ì¥ì ì„ ë§í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.

### 4.4 Interpolation

![](../../assets/img/Paper_Reading/DDPM/ddpm_25.jpg){: .normal}

ğŸ” Image space : $\mathbf{x}\_0, \mathbf{x}\_0^\prime \sim q(\mathbf{x}\_0)$

ğŸ” Diffused space : $\mathbf{x}\_t, \mathbf{x}\_t^\prime \sim q(\mathbf{x}\_t\|\mathbf{x}\_0)$

ğŸ” Linearly interpolated latent : $\bar{\mathbf{x}}\_t = (1-\lambda)\mathbf{x}\_0 + \lambda\mathbf{x}\_0^\prime$

ğŸ” Reverse processë¥¼ í†µí•´ image spaceë¡œ ë³´ë‚¸ ê²°ê³¼ (ìµœì¢… ê²°ê³¼) : $\bar{\mathbf{x}}\_0 \sim p(\mathbf{x}\_0\|\bar{\mathbf{x}}\_t)$
