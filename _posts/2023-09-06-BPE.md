---
layout: post
title: "[Tokenizer] Byte-Pair Encoding(BPE)"
author: kjy
date: 2023-09-06 20:15:00 +09:00
categories: [Tokenizer, NLP]
tags: [Tokenizer, NLP]
comments: true
toc: true
math: true
---

해당 포스트에서는 tokenizer의 한 종류인 BPE(Byte-Pair Encoding)에 대해 알아보겠습니다.

---

## BPE Algorithm

BPE Algorithm은 이미 오래 전에 제안되었던 데이터 압축 알고리즘이지만 OOV(Out-Of-Vocabulary)를 해결하기 위해 NLP분야에 도입되었습니다.

BPE Algorithm을 간단히 설명하자면 글자(character)단위에서 빈도수에 기반해서 점차적으로 단어 집합(vocabulary)을 만들어가는 알고리즘입니다.

더 자세한 방법은 아래의 예시를 통해 설명드리겠습니다.

## BPE Algorithm Training

1. Pre-tokenize

   Pre-tokenize를 띄어쓰기를 기준으로 했다고 했을 때의 결과가 아래와 같다고 가정을 하고 설명드리겠습니다.

   | Token | Frquency |
   | :---- | :------- |
   | hug   | 10       |
   | pug   | 5        |
   | pun   | 12       |
   | bun   | 4        |
   | hugs  | 5        |

2. Split character

   Initial(base) vocabulary가 `h`, `u`, `g`, `p`, `n`, `b`, `s`와 같다면 아래와 같이 위의 token들을 분리할 수 있습니다.

   | Token      | Frquency |
   | :--------- | :------- |
   | h, u, g    | 10       |
   | p, u, g    | 5        |
   | p, u, n    | 12       |
   | b, u, n    | 4        |
   | h, u, g, s | 5        |

3. Byte-Pair

   위에서 분리한 token 쌍의 빈도 수를 살펴봅니다. 이때 token 쌍을 Byte-Pair라고 부르며 연속된 두 token을 하나의 쌍으로 만든 것을 말합니다.

   | Byte-Pair | Frquency Formula                       | Frquency |
   | :-------- | :------------------------------------- | :------- |
   | h, u      | 10(**hu**g) + 5(**hu**gs)              | 15       |
   | u, g      | 10(h**ug**) + 5(p**ug**) + 5(h**ug**s) | 20       |
   | p, u      | 5(**pu**g) + 12(**pu**n)               | 17       |
   | u, n      | 12(p**un**) + 4(b**un**)               | 16       |
   | b, u      | 4(**bu**n)                             | 4        |
   | g, s      | 5(hu**gs**)                            | 5        |

4. Merge
   빈도 수가 가장 큰 `u, g`를 하나의 token으로 merge 합니다. 그럼 `h`, `u`, `g`, `p`, `n`, `b`, `s`, `ug`로 vocabulary가 update됩니다. 그 결과는 아래와 같습니다.

   | Token    | Frquency |
   | :------- | :------- |
   | h, ug    | 10       |
   | p, ug    | 5        |
   | p, u, n  | 12       |
   | b, u, n  | 4        |
   | h, ug, s | 5        |

5. Iteration
   이후 3번과 4번을 사용자가 정한 vocabulary 크기가 될 때까지 진행합니다.

   만약 사용자가 vocabulary 크기를 `10`으로 정했다면 아래와 같은 순서로 3번과 4번을 반복하니다.

   - Vocabulary : `h`, `u`, `g`, `p`, `n`, `b`, `s`, `ug`

     | Byte-Pair | Frquency Formula          | Frquency |
     | :-------- | :------------------------ | :------- |
     | h, ug     | 10(**hug**) + 5(**hug**s) | 15       |
     | p, ug     | 5 (**pug**)               | 5        |
     | p, u      | 12 (**pu**n)              | 12       |
     | u, n      | 12(p**un**) + 4(b**un**)  | 16       |
     | b, u      | 4 (**bu**n)               | 4        |
     | ug, s     | 5 (h**ugs**)              | 5        |

     | Token    | Frquency |
     | :------- | :------- |
     | h, ug    | 10       |
     | p, ug    | 5        |
     | p, un    | 12       |
     | b, un    | 4        |
     | h, ug, s | 5        |

   - Vocabulary : `h`, `u`, `g`, `p`, `n`, `b`, `s`, `ug`, `un`

     | Byte-Pair | Frquency Formula          | Frquency |
     | :-------- | :------------------------ | :------- |
     | h, ug     | 10(**hug**) + 5(**hug**s) | 15       |
     | p, ug     | 5 (**pug**)               | 5        |
     | p, un     | 12 (**pu**n)              | 12       |
     | b, un     | 4 (**bu**n)               | 4        |
     | ug, s     | 5 (h**ugs**)              | 5        |

     | Token   | Frquency |
     | :------ | :------- |
     | hug     | 10       |
     | p, ug   | 5        |
     | p, u, n | 12       |
     | b, u, n | 4        |
     | hug, s  | 5        |

   - Vocabulary : `h`, `u`, `g`, `p`, `n`, `b`, `s`, `ug`, `un`, `hug`

6. Result

   사용자가 정한 크기(`10`)의 vocabulary가 되면 아래와 같은 결과들을 얻을 수 있습니다.

   - Vocabulary : `h`, `u`, `g`, `p`, `n`, `b`, `s`, `ug`, `un`, `hug`
   - Merge order: `ug` ➡️ `un` ➡️ `hug`

## BPE Algorithm Apply

위에서 학습한 BPE Algorithm을 `pug bug mug`에 적용해 보겠습니다.

1. Pre-tokenize

   - `pug bug mug` ➡️ `pug`, `bug`, `mug`

2. Split character

   - `pug` ➡️ `p`, `u`, `g`
   - `bug` ➡️ `b`, `u`, `g`
   - `mug` ➡️ `m`, `u`, `g`

3. Byte-Pair

   - `pug` ➡️ `p`, `u`, `g` ➡️ `pu`, `ug`
   - `bug` ➡️ `b`, `u`, `g` ➡️ `bu`, `ug`
   - `mug` ➡️ `m`, `u`, `g` ➡️ `mu`, `ug`

4. Compare merge order & Merge

   해당 순서에서는 merge order를 비교하고 우선순위에 있는 byte-pair를 먼저 merge 하고 merge가 불가능할 때까지 merge를 해주면 됩니다. 그러나 만약 merge order에 비교하려는 쌍이 없는 경우 우선순위가 없는 것으로 판단하여 merge가 불가능한 경우로 판단하면 됩니다.

   > Merge order: `ug` ➡️ `un` ➡️ `hug`

   - `pug` ➡️ `p`, `u`, `g` ➡️ `pu`, `ug` ➡️ `p`, `ug` (더 이상 merge 불가능)
   - `bug` ➡️ `b`, `u`, `g` ➡️ `bu`, `ug` ➡️ `b`, `ug` (더 이상 merge 불가능)
   - `mug` ➡️ `m`, `u`, `g` ➡️ `mu`, `ug` ➡️ `m`, `ug` (더 이상 merge 불가능)

5. Check in vocabulary

   위에서 찾은 쌍들이 vocabulary에 있는지 확인해야 합니다. 만약 vocabulary에 없는 경우 `<unk>` 토큰을 부여합니다.

   > Vocabulary : `h`, `u`, `g`, `p`, `n`, `b`, `s`, `ug`, `un`, `hug`

   - `pug` ➡️ `p`, `u`, `g` ➡️ `pu`, `ug` ➡️ `p`, `ug` ➡️ `p`, `ug`
   - `bug` ➡️ `b`, `u`, `g` ➡️ `bu`, `ug` ➡️ `b`, `ug` ➡️ `b`, `ug`
   - `mug` ➡️ `m`, `u`, `g` ➡️ `mu`, `ug` ➡️ `m`, `ug` ➡️ `<unk>`, `ug`

# ※ Reference

- [https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/](https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/)
- [https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)
- [https://wikidocs.net/22592](https://wikidocs.net/22592)
